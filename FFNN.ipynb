{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from typing import List\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import dill\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# Untuk downscale gambar, maaf kak, laptop kentang kami menderita karena ukuran 28x28 \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, value, _children=(), _op='', label=''):\n",
    "        self.value = value\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label # for visualization\n",
    "\n",
    "    def __repr__(self):\n",
    "        stringVal = f\"{self.value}\"\n",
    "        return stringVal\n",
    "\n",
    "    def __add__(self,other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        out = Neuron(self.value + other.value, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __mul__(self,other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        out = Neuron(self.value * other.value, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers\"\n",
    "        out = Neuron(self.value**other, (self, ), f'**{other}')\n",
    "        def _backward():\n",
    "            self.grad += other * (self.value ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        return self * other**-1\n",
    "\n",
    "    def exp(self, pos=1):\n",
    "        x = self.value*pos\n",
    "        out = Neuron(exp(x), (self, ), 'exp')\n",
    "        def _backward():\n",
    "            self.grad += out.value * out.grad * pos\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def log(self):\n",
    "        x = self.value\n",
    "        out = Neuron(np.log(x), (self,), 'log')\n",
    "        def _backward():\n",
    "            self.grad += (1/x) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return -(self - other)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        return other * self**-1\n",
    "\n",
    "    def __lt__(self,other):\n",
    "        if(isinstance(other,Neuron)):\n",
    "            return self.value < other.value\n",
    "        else:\n",
    "            return self.value < other\n",
    "\n",
    "    def __gt__(self,other):\n",
    "        if(isinstance(other,Neuron)):\n",
    "            return self.value > other.value\n",
    "        else:\n",
    "            return self.value > other\n",
    "\n",
    "    def __le__(self,other):\n",
    "        if(isinstance(other,Neuron)):\n",
    "            return self.value <= other.value\n",
    "        else:\n",
    "            return self.value <= other\n",
    "\n",
    "    def __ge__(self,other):\n",
    "        if(isinstance(other,Neuron)):\n",
    "            return self.value >= other.value\n",
    "        else:\n",
    "            return self.value >= other\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        topology = []\n",
    "        visited = set()\n",
    "        def build_topology(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topology(child)\n",
    "                topology.append(v)\n",
    "        build_topology(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topology):\n",
    "            node._backward()\n",
    "\n",
    "    def __float__(self):\n",
    "        return float(self.value)\n",
    "\n",
    "    def __format__(self, other):\n",
    "        return format(float(self), other)\n",
    "\n",
    "    def __round__(self,other):\n",
    "        return round(self.value,other)\n",
    "\n",
    "    def getGrad(self):\n",
    "        return float(self.grad)\n",
    "\n",
    "    def hello(self):\n",
    "        print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper, propose better name?\n",
    "# Maybe MLP? (multi layer perceptron, since it is a wrapper for mutliple layers of neurons)\n",
    "class LayerWrapper:\n",
    "    valid_loss_func = {\"mse\",\"bce\",\"cce\"}\n",
    "\n",
    "    def __init__(self,\n",
    "                 layer_neuron_width:List[int],\n",
    "                 activation_function:str = \"linear\",\n",
    "                 weight_initialization:str = \"uniform\",\n",
    "                 loss_function:str = \"mse\",\n",
    "                 uniform_parameter: tuple = (0, 10, 0),\n",
    "                 normal_parameter: tuple = (5, 3, 0),\n",
    "                 batch_size: int = 1,\n",
    "                 learning_rate: float = 0.1,\n",
    "                 max_epoch: int = 1,\n",
    "                 verbose: int = 0,\n",
    "                 regularization_type: str = \"Unregularized\",\n",
    "                 regularization_lambda: float = 0\n",
    "                 ):\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_function = loss_function\n",
    "        self.layer_selector:List[Layers] = []\n",
    "        self.layer_count = len(layer_neuron_width)\n",
    "        self.loss = Neuron(0)\n",
    "        self.error_acc = Neuron(0)\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epoch = max_epoch\n",
    "        self.verbose = verbose\n",
    "        self.regularization_type: str = regularization_type,\n",
    "        self.regularization_lambda: float = regularization_lambda\n",
    "        Layers(\n",
    "            wrapper=self,\n",
    "            layer_neuron_width=layer_neuron_width,\n",
    "            activation_function=activation_function,\n",
    "            weight_initialization=weight_initialization,\n",
    "            uniform_parameter=uniform_parameter,\n",
    "            normal_parameter=normal_parameter\n",
    "        )\n",
    "\n",
    "        self.train_class_value = LayerWrapper.neuronGenerator(np.zeros,(len(self.layer_selector[-1]._neurons)))\n",
    "        self.validation_class_value = LayerWrapper.neuronGenerator(np.zeros,(len(self.layer_selector[-1]._neurons)))\n",
    "\n",
    "    def verboseprint(self, *args: str, nl: bool=True):\n",
    "        print(*args, end='\\n' if nl else '\t') if self.verbose else None\n",
    "\n",
    "    def save(self,filename):\n",
    "        with open(f\"{filename}\",\"wb\") as file:\n",
    "            dill.dump(self,file)\n",
    "\n",
    "    def load(self,filename):\n",
    "        with open(f\"{filename}\",\"rb\") as file:\n",
    "            self = dill.load(file)\n",
    "\n",
    "    # Function, dengan retval\n",
    "    @staticmethod\n",
    "    def neuronGenerator(generator, *args, **kwargs):\n",
    "        value = generator(*args, **kwargs)\n",
    "        return np.vectorize(Neuron)(value)\n",
    "\n",
    "    def loss_func(self,Y_pred,Y_target) -> float:\n",
    "        if self.loss_function == \"mse\":\n",
    "            return LayerWrapper.meanSquareError(Y_pred,Y_target)\n",
    "        elif self.loss_function == \"bce\":\n",
    "            return LayerWrapper.binaryCrossEntropy(Y_pred,Y_target)\n",
    "        elif self.loss_function == \"cce\":\n",
    "            return LayerWrapper.categoricalCrossEntropy(Y_pred,Y_target)\n",
    "\n",
    "    @staticmethod\n",
    "    def meanSquareError(Y_pred,Y_target) -> float:\n",
    "        tempPredictClass = Y_pred\n",
    "        tempTrueClass = Y_target\n",
    "        subtracted = np.subtract(tempTrueClass,tempPredictClass)\n",
    "        squared = np.square(subtracted)\n",
    "        return np.sum(squared)/len(tempTrueClass)\n",
    "\n",
    "    @staticmethod\n",
    "    def binaryCrossEntropy(Y_pred: \"Layers\",Y_target) -> float:\n",
    "        tempPredictClass = Y_pred\n",
    "        tempTrueClass = Y_target\n",
    "        logFunc = np.vectorize(lambda n: n.log())\n",
    "        oneMinTarget = np.subtract(1, Y_target)\n",
    "        # print(tempPredictClass)\n",
    "        oneMinPredict = np.subtract(1, tempPredictClass)\n",
    "        logPredictedClass = logFunc(tempPredictClass)\n",
    "        logOneMinPredict = logFunc(oneMinPredict)\n",
    "        return -np.sum(np.add(tempTrueClass*logPredictedClass, oneMinTarget*logOneMinPredict))/len(tempTrueClass)\n",
    "\n",
    "    @staticmethod\n",
    "    def categoricalCrossEntropy(Y_pred: \"Layers\",Y_target) -> float:\n",
    "        # Categorical Cross Entropy target will be an array cause the target will be encoded\n",
    "        # For example if we have 3 classes 0, 1, 2 and the target for this instance is 1, then the target will be [0, 1, 0]\n",
    "        # The predict or the output we get must be probability for every classes in target\n",
    "        # This error formula only can be used with softmax activation function\n",
    "        encoder = OneHotEncoder(sparse_output=False)\n",
    "        tempTrueClass = encoder.fit_transform(np.array(Y_target).reshape(-1, 1))\n",
    "        tempPredictClass = encoder.fit_transform(np.array(Y_pred).reshape(-1, 1))\n",
    "        # tempPredictClass = Y_pred   # ingat ini array of list yh, nvm better gua encode di dalem sini aja\n",
    "        # tempTrueClass = Y_target    # ini juga yh\n",
    "        logFunc = np.vectorize(lambda n: n.log())\n",
    "        logPredictedClass = logFunc(tempPredictClass)\n",
    "        return -np.sum(np.multiply(tempTrueClass, logPredictedClass))/len(tempTrueClass)\n",
    "\n",
    "\n",
    "    def predict(self,inputVal:List[float]):\n",
    "        # need progress bar in this one too\n",
    "        for x,y in zip(self.layer_selector[0],inputVal):\n",
    "            x.value = y\n",
    "        self.feedforward()\n",
    "        return self.getPredResult()\n",
    "\n",
    "    def getPredResult(self):\n",
    "        return self.layer_selector[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.layer_selector[idx]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.layer_selector}\"\n",
    "\n",
    "\n",
    "    # Procedure, getter, setter or something else\n",
    "\n",
    "    def regularization(self):\n",
    "            sum = 0\n",
    "            if self.regularization_type == \"L1\":\n",
    "                for layer in self.layer_selector:\n",
    "                    abs_val = np.abs(layer)\n",
    "                    sum += np.sum(abs_val)\n",
    "            elif self.regularization_type == \"L2\":\n",
    "                for layer in self.layer_selector:\n",
    "                    square_val = np.square(layer)\n",
    "                    sum += np.sum(square_val)\n",
    "            self.loss += self.regularization_lambda*sum  # error_accumulate atau loss, idk need to test it\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.layer_selector[0].feedforward()\n",
    "\n",
    "    def batchHelper(self,\n",
    "                batch_input: np.ndarray,\n",
    "                batch_class: np.ndarray,\n",
    "                ):\n",
    "        self.loss = Neuron(0)\n",
    "        self.error_acc = 0\n",
    "        total_samples = len(batch_input)\n",
    "        i = 0\n",
    "        while i < total_samples:\n",
    "            self.loss = Neuron(0)\n",
    "            batch_end = min(i + self.batch_size, total_samples)\n",
    "            for j in range(i, batch_end):\n",
    "                input_sample = list(batch_input[j])\n",
    "                target_sample = list(batch_class[j])\n",
    "                self.layer_selector[0].setLayerNeurons(input_sample)\n",
    "                self.layer_selector[0].feedforward()\n",
    "                Y_pred = self.layer_selector[-1]._neurons\n",
    "                Y_target = self.neuronGenerator(np.array, target_sample)\n",
    "                loss = self.loss_func(Y_pred=Y_pred, Y_target=Y_target)\n",
    "                self.loss += loss\n",
    "                self.error_acc += loss\n",
    "            self.regularization()\n",
    "            self.loss.backward()\n",
    "            self.updateWeightHelper()\n",
    "            i += self.batch_size\n",
    "\n",
    "    def validate(self,X_val,Y_val):\n",
    "        loss = 0\n",
    "        while(len(Y_val)>0):\n",
    "            X_val_current =  list(X_val[0])\n",
    "            Y_val_current = list(Y_val[0])\n",
    "            self.layer_selector[0].setLayerNeurons(X_val_current)\n",
    "            self.layer_selector[0].feedforward()\n",
    "            Y_pred = self.layer_selector[-1]._neurons\n",
    "            Y_target = self.neuronGenerator(np.array,(Y_val_current))\n",
    "            loss += self.loss_func(Y_pred=Y_pred,Y_target=Y_target)\n",
    "            X_val=X_val[1:]\n",
    "            Y_val=Y_val[1:]\n",
    "        return loss\n",
    "\n",
    "    def epoch(self,X_train,Y_train,X_val,Y_val):\n",
    "        X_train = np.array(X_train)\n",
    "        Y_train = np.array(Y_train)\n",
    "        X_val = np.array(X_val)\n",
    "        Y_val = np.array(Y_val)\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        for i in (tqdm(range(self.max_epoch), desc=\"Epoch\") if self.verbose else range(self.max_epoch)):\n",
    "            # print(f\"Epoch {i}\")\n",
    "            self.batchHelper(\n",
    "                batch_class=Y_train,\n",
    "                batch_input=X_train\n",
    "            )\n",
    "            val = self.validate(X_val=X_val,Y_val=Y_val)\n",
    "            tl_val = self.error_acc/len(Y_train)\n",
    "            vl_val = val/len(Y_val)\n",
    "            train_loss.append(tl_val)\n",
    "            self.error_acc = Neuron(0)\n",
    "            val_loss.append(vl_val)\n",
    "\n",
    "            self.verboseprint(\"Training Loss:\", tl_val, nl=False)\n",
    "            self.verboseprint(\"Validation Loss:\", vl_val, nl=True)\n",
    "            # print(train_loss[i],val_loss[i])\n",
    "        self.history = [train_loss,val_loss]\n",
    "        return self.history\n",
    "\n",
    "    def fit(self,X_train,Y_train,X_val,Y_val):\n",
    "        return self.epoch(X_train,Y_train,X_val,Y_val)\n",
    "\n",
    "    def updateWeightHelper(self):\n",
    "        self.layer_selector[0].updateWeightRecursive()\n",
    "\n",
    "    def visualizeGraph(self,size=(30,15)):\n",
    "        self.layer_selector[0].visualizeGraph(size=size)\n",
    "\n",
    "    def visualizeWeightDist(self,indices):\n",
    "        for i in indices:\n",
    "            sns.displot(self.layer_selector[i].weight.flatten().astype(float)).set(title=f\"Weight Layer-{i}\")\n",
    "\n",
    "    def visualizeWeightGradDist(self,indices):\n",
    "        for i in indices:\n",
    "            grad_data = np.array([item.grad for item in self.layer_selector[i].weight.flatten()])\n",
    "            sns.displot(grad_data.astype(float)).set(title=f\"Weight Grad Layer-{i}\")\n",
    "\n",
    "    # Kebodohan: backward untuk grad hanya boleh dipanggil 1x dari 1 titik, kalau kyk\n",
    "    # gini dipanggil berkali kali dan akhirnya refer ke node yg terakhir di layer\n",
    "    # terakhir\n",
    "    # *ada alasan kenapa yg dibackprop cuma errornya doang\n",
    "    # def backpropagation(self):\n",
    "    #     backprop = np.vectorize(lambda x: x.backward())\n",
    "    #     backprop(self.layer_selector[-1]._neurons)\n",
    "    # Tidak dihapus untuk catatan kebodohan\n",
    "\n",
    "\n",
    "class Layers:\n",
    "    # Static Attribute, cuma buat validasi input\n",
    "    valid_activation_function = {\"linear\",\"relu\",\"sigmoid\",\"hyperbolic_tangent\",\"softmax\",\"leaky_relu\",\"swish\"}\n",
    "    valid_weight_initialization = {\"zero\",\"uniform\",\"normal\"}\n",
    "\n",
    "    # START OF INITIALIZATION\n",
    "    def __init__(self,\n",
    "                 wrapper:LayerWrapper,                      # Wrapper ini buat ngepoint ke wrappernya\n",
    "                 layer_neuron_width:list,                   # List of neuron width, elemen index ke-n menentukan lebar layer ke-n\n",
    "                #  activation_function:str = \"linear\",        # Harus ada di atas, kubuat defaultnya ini, dan rekursif layer selanjutnya sama\n",
    "                 activation_function:list,                  # activation function untuk setiap layer, elemen idx ke-n merupakan activation function untuk layer ke-n+1 (idx 0 = input_layer -> layer 1)\n",
    "                 weight_initialization:list,     # Sama kyk activation function\n",
    "                 current_iter:int = 0,                      # jgn disentuh, ini buat nandain iterasinya, krn inisialisasi rekursif\n",
    "                 uniform_parameter: tuple = (0,10,0),       # Hanya terpakai kalau activation_functionnya uniform\n",
    "                 normal_parameter: tuple = (5,3,0),         # kyk di atas, tp normal\n",
    "\n",
    "                 # Tambah parameter kasih nama, named parameter biar ga broken pemanggilannya\n",
    "                 ):\n",
    "\n",
    "        # Cek validasi\n",
    "        self.inputValidation(\n",
    "            func_name=activation_function,\n",
    "            weight_name=weight_initialization,\n",
    "            layer_neuron_width=layer_neuron_width\n",
    "            )\n",
    "\n",
    "        self.wrapper        : LayerWrapper                      = wrapper\n",
    "        self.current_iter   : int                               = current_iter\n",
    "        self.neuron_count   : int                               = layer_neuron_width[current_iter]\n",
    "        self.bias_weight    : Neuron                            = Neuron(0)\n",
    "        self._neurons       : np.ndarray[any, np.dtype[Neuron]] = self.neuronGenerator(np.zeros,(self.neuron_count))\n",
    "\n",
    "        if current_iter < len(layer_neuron_width)-1:\n",
    "            self.weight_initialization  : str    = weight_initialization[current_iter]\n",
    "        else:\n",
    "            self.weight_initialization = None\n",
    "\n",
    "        self.uniform_parameter  : tuple = uniform_parameter\n",
    "        self.normal_parameter   : tuple = normal_parameter\n",
    "\n",
    "        if current_iter > 0:\n",
    "            self.activation_function    : str   = activation_function[current_iter-1]\n",
    "        else:\n",
    "            self.activation_function = None\n",
    "\n",
    "        wrapper.layer_selector.append(self)\n",
    "        self.next = (\n",
    "            Layers(\n",
    "                wrapper=wrapper,\n",
    "                layer_neuron_width=layer_neuron_width,\n",
    "                activation_function=activation_function,\n",
    "                current_iter=current_iter + 1,\n",
    "                weight_initialization=weight_initialization,\n",
    "                uniform_parameter=uniform_parameter,\n",
    "                normal_parameter=normal_parameter\n",
    "                )\n",
    "            if current_iter < len(layer_neuron_width) - 1\n",
    "            else None\n",
    "        )\n",
    "        self.initWeight()\n",
    "    # END OF INITIALIZATION\n",
    "\n",
    "    # Buat ngeprint\n",
    "    def __repr__(self):\n",
    "        return f\"{self._neurons}\"\n",
    "\n",
    "    # Kadang reflek Layer[0] dan dapet error, mending lgsg keluarin Neuron nya\n",
    "    def __getitem__(self, idx):\n",
    "        return self._neurons[idx]\n",
    "\n",
    "    def __setitem__(self, idx, val):\n",
    "        if (isinstance(val,Neuron)):\n",
    "            self._neurons[idx] = val\n",
    "        else:\n",
    "            self._neurons[idx] = Neuron(val)\n",
    "\n",
    "    # Kalau ada yg aku miss tambahin aja\n",
    "    def inputValidation(self,\n",
    "                        func_name:list,\n",
    "                        weight_name:list,\n",
    "                        layer_neuron_width:list\n",
    "                        ):\n",
    "\n",
    "        if any(el not in Layers.valid_activation_function for el in func_name):\n",
    "            raise ValueError(f\"Valid activation function name: {Layers.valid_activation_function}\")\n",
    "\n",
    "        if any(el  not in Layers.valid_weight_initialization for el in weight_name):\n",
    "            raise ValueError(f\"Valid weight initialization: {Layers.valid_weight_initialization}\")\n",
    "\n",
    "        if len(layer_neuron_width) < 1:\n",
    "            raise ValueError(f\"layer_neuron_width must be a list with at least one positive integer.\")\n",
    "\n",
    "        if not all(isinstance(n, int) and n > 0 for n in layer_neuron_width):\n",
    "            raise ValueError(f\"All elements in layer_neuron_width must be a positive integer\")\n",
    "\n",
    "    # Init weightnya ngikut dari named parameter\n",
    "    def initWeight(self):\n",
    "        if self.weight_initialization==\"normal\":\n",
    "            mean        = self.normal_parameter[0]\n",
    "            variance    = self.normal_parameter[1]\n",
    "            seed        = self.normal_parameter[2]\n",
    "            self.initWeightNormal(mean,variance,seed)\n",
    "\n",
    "        elif self.weight_initialization==\"uniform\":\n",
    "            low     = self.uniform_parameter[0]\n",
    "            high    = self.uniform_parameter[1]\n",
    "            seed    = self.uniform_parameter[2]\n",
    "            self.initWeightUniform(low,high,seed)\n",
    "\n",
    "        elif self.weight_initialization==\"zero\":\n",
    "            self.initWeightZero()\n",
    "\n",
    "    # Tiga function di bawah buat initialization, buat manual assign per layer\n",
    "    def initWeightUniform(\n",
    "            self,\n",
    "            low     : float =   0,\n",
    "            high    : float =   10,\n",
    "            seed    : int   =   0\n",
    "            ):\n",
    "\n",
    "        if self.next != None:\n",
    "            self.weight_initialization = \"uniform\"\n",
    "            rng = np.random.default_rng(seed=seed)\n",
    "            next_neuron_count = self.next.neuron_count\n",
    "            self.weight = self.neuronGenerator(rng.uniform,low,high,size=(next_neuron_count,self.neuron_count))\n",
    "\n",
    "    def initWeightNormal(\n",
    "            self,\n",
    "            mean        :   float   =   5,\n",
    "            variance    :   float   =   3,\n",
    "            seed        :   int     =   0\n",
    "            ):\n",
    "        if self.next != None:\n",
    "            self.weight_initialization = \"normal\"\n",
    "            rng = np.random.default_rng(seed=seed)\n",
    "            next_neuron_count = self.next.neuron_count\n",
    "            self.weight = self.neuronGenerator(rng.normal,loc=mean,scale=variance*variance,size=(next_neuron_count,self.neuron_count))\n",
    "\n",
    "    def initWeightZero(\n",
    "            self\n",
    "        ):\n",
    "        if self.next != None:\n",
    "            self.weight_initialization = \"zero\"\n",
    "            next_neuron_count = self.next.neuron_count\n",
    "            self.weight = self.neuronGenerator(np.zeros,shape=(next_neuron_count,self.neuron_count))\n",
    "\n",
    "    # Feedforward rekursif, biar ga tolol kyk sebelumnya, mohon maaf\n",
    "    def feedforward(self):\n",
    "        if self.next != None:\n",
    "            self.next._neurons = np.dot(self._neurons,self.weight.T) + self.bias_weight\n",
    "            self.next._neurons = self.activate(self.next._neurons)\n",
    "            self.next.feedforward()\n",
    "\n",
    "    # Update weight\n",
    "    def updateWeightRecursive(self):\n",
    "        if self.next is not None:\n",
    "            self.weight = Layers.vectorUpdateWeight(self.weight, self.wrapper.learning_rate)\n",
    "            self.bias_weight = self.bias_weight - (self.wrapper.learning_rate*self.bias_weight.grad)\n",
    "            for i in self.weight:\n",
    "                for y in i:\n",
    "                    y._prev = set()\n",
    "            self.bias_weight._prev = set()\n",
    "            self.next.updateWeightRecursive()\n",
    "\n",
    "    # set Layer ini ngisi manual node nya, baru sadar ga guna jg sih\n",
    "    # node hidden layer bakal kereplace jg pas feedforward\n",
    "    def setLayerNeurons(\n",
    "            self,\n",
    "            neuron_list:List[float]\n",
    "            ):\n",
    "        prev_layer_count = len(self._neurons)\n",
    "        self._neurons = self.neuronGenerator(np.array,(neuron_list))\n",
    "        if self.current_iter>0:\n",
    "            if prev_layer_count != len(self._neurons):\n",
    "                self.wrapper.layer_selector[self.current_iter-1].initWeight()\n",
    "        if prev_layer_count!=len(self._neurons):\n",
    "            self.initWeight()\n",
    "\n",
    "    # Set lebar/jumlah neuron dari 1 layer\n",
    "    # di bawah ada set weight karena weight depend on next hidden layer width\n",
    "    def setLayerWidth(\n",
    "            self,\n",
    "            width: int = 1,\n",
    "        ):\n",
    "        self._neurons = self.neuronGenerator(np.zeros,(width))\n",
    "        self.neuron_count = width\n",
    "        if self.current_iter>0:\n",
    "            self.wrapper.layer_selector[self.current_iter-1].initWeight()\n",
    "        self.initWeight()\n",
    "\n",
    "    def setBias(self, bias:float):\n",
    "        self.bias_weight = Neuron(bias)\n",
    "\n",
    "    # Masih sama dgn kode lama, buat fill in neuron di numpy\n",
    "    def neuronGenerator(self,generator, *args, **kwargs):\n",
    "        value = generator(*args, **kwargs)\n",
    "        return np.vectorize(Neuron)(value)\n",
    "\n",
    "    @staticmethod\n",
    "    def vectorUpdateWeight(inputVal:Neuron,rate:float):\n",
    "        def updateSingleNeuron(inputVal:Neuron,rate:float):\n",
    "            inputVal._prev = set()\n",
    "            return inputVal-(rate*inputVal.grad)\n",
    "        updateTemp = np.vectorize(updateSingleNeuron)\n",
    "        return updateTemp(inputVal,rate)\n",
    "\n",
    "    # ACTIVATION FUNCTION\n",
    "\n",
    "    def activate(self,input):\n",
    "        if self.next.activation_function==\"linear\":\n",
    "            return Layers.linear(input)\n",
    "        elif self.next.activation_function==\"relu\":\n",
    "            return Layers.relu(input)\n",
    "        elif self.next.activation_function==\"sigmoid\":\n",
    "            return Layers.sigmoid(input)\n",
    "        elif self.next.activation_function==\"hyperbolic_tangent\":\n",
    "            return Layers.hyperbolic_tangent(input)\n",
    "        elif self.next.activation_function==\"softmax\":\n",
    "            return Layers.soft_max(input)\n",
    "        elif self.next.activation_function==\"swish\":\n",
    "            return Layers.swish(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def linear(inputVal):\n",
    "        return inputVal\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(inputVal):\n",
    "        reluTemp = np.vectorize(lambda x: x if x > 0 else Neuron(0))\n",
    "        return reluTemp(inputVal)\n",
    "\n",
    "    # TODO: add parameter, alphanya masih hardcoded 0.5\n",
    "    def leaky_relu(self, alpha=0.01):\n",
    "        leakytemp = np.vectorize(lambda x: x if x > 0 else Neuron(0)*Neuron(alpha))\n",
    "        return leakytemp(self._neurons)\n",
    "\n",
    "    # TODO: add parameter, beta nya masih hardcoded 1\n",
    "    @staticmethod\n",
    "    def swish(inputVal):\n",
    "        def sigmoidScalar(x:Neuron):\n",
    "            return x/(1+x.exp(-1))\n",
    "\n",
    "        vectorized_sigmoid = np.vectorize(sigmoidScalar)\n",
    "        return vectorized_sigmoid(inputVal)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(inputVal):\n",
    "        def sigmoidScalar(x:Neuron):\n",
    "            return 1/(1+x.exp(-1))\n",
    "\n",
    "        vectorized_sigmoid = np.vectorize(sigmoidScalar)\n",
    "        return vectorized_sigmoid(inputVal)\n",
    "\n",
    "    @staticmethod\n",
    "    def hyperbolic_tangent(inputVal):\n",
    "        def tanhScalar(x:Neuron):\n",
    "            return (x.exp()-x.exp(-1))/(x.exp()+x.exp(-1))\n",
    "\n",
    "        vectorized_tanh = np.vectorize(tanhScalar)\n",
    "        return vectorized_tanh(inputVal)\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_max(inputVal):\n",
    "        # Softmax output layers will have the same number as the number of output class\n",
    "        exp = np.vectorize(lambda x: x.exp())\n",
    "        divide = np.vectorize(lambda x,y: x/y)\n",
    "        temp = exp(inputVal)\n",
    "        sumTemp = np.sum(temp)\n",
    "        return divide(temp,sumTemp)\n",
    "\n",
    "    def visualizeGraph(self, index=0, G=nx.DiGraph(), layers=[], size=(30, 15)):\n",
    "        if self.next is not None:\n",
    "            current_layer_nodes = [f\"Layer{index} node {x}\" for x in range(len(self._neurons))]\n",
    "            next_layer_nodes = [f\"Layer{index+1} node {x}\" for x in range(len(self.next._neurons))]\n",
    "            current_weights = [[f\"{w:.2f}\" for w in node] for node in self.weight]\n",
    "            current_weights_grad = [[f\"{w.grad:.2f}\" for w in node] for node in self.weight]\n",
    "            bias_node = f\"BiasLayer {index}\"\n",
    "            current_layer_nodes.append(bias_node)\n",
    "            layers.append(current_layer_nodes)\n",
    "\n",
    "\n",
    "            for i in range(len(current_layer_nodes)):\n",
    "                for j in range(len(next_layer_nodes)):\n",
    "                    if current_layer_nodes[i]==f\"BiasLayer {index}\":\n",
    "                        G.add_edge(current_layer_nodes[i],next_layer_nodes[j],\n",
    "                                   weight=self.bias_weight.value,\n",
    "                                   grad=self.bias_weight.grad\n",
    "                                   )\n",
    "                    else:\n",
    "                        G.add_edge(current_layer_nodes[i], next_layer_nodes[j],\n",
    "                                weight=current_weights[j][i],\n",
    "                                grad=current_weights_grad[j][i])\n",
    "\n",
    "            self.next.visualizeGraph(index + 1, G, layers, size)\n",
    "\n",
    "        else:\n",
    "            last_layer_nodes = [f\"Layer{index} node {x}\" for x in range(len(self._neurons))]\n",
    "            layers.append(last_layer_nodes)\n",
    "\n",
    "            pos = {}\n",
    "            for layer_idx, layer_nodes in enumerate(layers):\n",
    "                for i, node in enumerate(layer_nodes):\n",
    "                    pos[node] = (layer_idx, -i)\n",
    "\n",
    "            plt.figure(figsize=size)\n",
    "            nx.draw(G, pos, with_labels=True, node_color='lightgreen',\n",
    "                    node_size=2000, font_size=14, arrows=True)\n",
    "\n",
    "            edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "            nx.draw_networkx_edge_labels(\n",
    "                G,\n",
    "                pos,\n",
    "                edge_labels=edge_labels,\n",
    "                font_color='blue',\n",
    "                label_pos=0.75,\n",
    "                bbox=None\n",
    "            )\n",
    "\n",
    "            edge_labels2 = nx.get_edge_attributes(G, 'grad')\n",
    "            nx.draw_networkx_edge_labels(\n",
    "                G,\n",
    "                pos,\n",
    "                edge_labels=edge_labels2,\n",
    "                font_color='red',\n",
    "                label_pos=0.25,\n",
    "                bbox=None\n",
    "            )\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xorFunc = LayerWrapper(\n",
    "    activation_function=[\"sigmoid\", \"sigmoid\", \"sigmoid\"],\n",
    "    weight_initialization=[\"uniform\", \"uniform\", \"uniform\"],\n",
    "    loss_function=\"mse\",\n",
    "    layer_neuron_width=[2,4,4,1],\n",
    "    batch_size=1,\n",
    "    learning_rate=0.9,\n",
    "    max_epoch=4500,\n",
    "\tverbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = xorFunc.fit(\n",
    "    Y_train=[[1],[1],[0],[0]],\n",
    "    X_train=[[1,0],[0,1],[1,1],[0,0]],\n",
    "    X_val=[[1,0],[1,1]],\n",
    "    Y_val=[[1],[0]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xorFunc.visualizeGraph((12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(xorFunc.history[0]))\n",
    "\n",
    "plt.plot(x, xorFunc.history[0],label='Train')\n",
    "plt.plot(x, xorFunc.history[1], label='Val')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=1000, test_size=200)\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "X_train = tf.reshape(X_train, [-1, 28, 28, 1])\n",
    "X_test = tf.reshape(X_test, [-1, 28, 28, 1])\n",
    "\n",
    "X_train = tf.image.resize(X_train, [14, 14])\n",
    "X_test = tf.image.resize(X_test, [14, 14])\n",
    "\n",
    "X_train = tf.reshape(X_train, [X_train.shape[0], -1])\n",
    "X_test = tf.reshape(X_test, [X_test.shape[0], -1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
