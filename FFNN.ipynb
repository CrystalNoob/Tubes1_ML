{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, value, _children=(), _op='', label=''):\n",
    "        self.value = value\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children);\n",
    "        self._op = _op;\n",
    "        self.label = label # for visualization\n",
    "\n",
    "    def __repr__(self):\n",
    "        stringVal = f\"{self.value}\";\n",
    "        return stringVal\n",
    "\n",
    "    def __add__(self,other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        out = Neuron(self.value + other.value, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __mul__(self,other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        out = Neuron(self.value * other.value, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers\"\n",
    "        out = Neuron(self.value**other, (self, ), f'**{other}')\n",
    "        def _backward():\n",
    "            self.grad += other * (self.value ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        return self * other**-1\n",
    "\n",
    "    def exp(self, pos=1):\n",
    "        x = self.value*pos\n",
    "        out = Neuron(math.exp(x), (self, ), 'exp')\n",
    "        def _backward():\n",
    "            self.grad += out.value * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return -(self - other)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return self / other**-1\n",
    "\n",
    "    def backward(self):\n",
    "        topology = []\n",
    "        visited = set()\n",
    "        def build_topology(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topology(child)\n",
    "                topology.append(v)\n",
    "        build_topology(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topology):\n",
    "            node._backward()\n",
    "\n",
    "    def __float__(self):\n",
    "        return float(self.value)\n",
    "    \n",
    "    def getGrad(self):\n",
    "        return float(self.grad)\n",
    "\n",
    "    def hello(self):\n",
    "        print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerWrapper:\n",
    "    def __init__(self,\n",
    "                 layer_neuron_count:list,\n",
    "                 activation_function:str=\"linear\",\n",
    "                 ):\n",
    "        self.layer_selector = []\n",
    "        self.layer_neuron_count = layer_neuron_count\n",
    "        Layers(\n",
    "            self,\n",
    "            layer_neuron_count,\n",
    "            activation_function\n",
    "        )\n",
    "        \n",
    "\n",
    "class Layers:\n",
    "    valid_activation_function = {\"linear\",\"relu\",\"sigmoid\",\"hyperbolic_tangent\",\"softmax\"}\n",
    "    def __init__(self,\n",
    "                 wrapper:LayerWrapper,\n",
    "                 layer_neuron_count:list,\n",
    "                 activation_function:str = \"linear\",\n",
    "                 current_iter:int = 0,\n",
    "                 ):\n",
    "        \n",
    "        self.checkActivationFunction(activation_function)\n",
    "        self.wrapper = wrapper\n",
    "        self.current_iter = current_iter\n",
    "        self.neuron_count = layer_neuron_count[current_iter]\n",
    "        self.bias_weight = Neuron(0)\n",
    "        self.neurons = self.neuronGenerator(np.zeros,(self.neuron_count))\n",
    "        wrapper.layer_selector.append(self)\n",
    "        self.next = (\n",
    "            Layers(wrapper,layer_neuron_count,activation_function,current_iter + 1)\n",
    "            if current_iter < len(layer_neuron_count) - 1\n",
    "            else None\n",
    "        )\n",
    "        self.initWeightUniform()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.neurons}\"\n",
    "    \n",
    "    def checkActivationFunction(self,name:str):\n",
    "        if name not in Layers.valid_activation_function:\n",
    "            raise ValueError(f\"Valid activation function name: {Layers.valid_activation_function}\")\n",
    "\n",
    "    def initWeightUniform(\n",
    "            self,\n",
    "            low=0,\n",
    "            high=10,\n",
    "            seed=0\n",
    "            ):\n",
    "        if self.next != None:\n",
    "            rng = np.random.default_rng(seed=seed)\n",
    "            next_neuron_count = self.next.neuron_count\n",
    "            self.weight = self.neuronGenerator(rng.uniform,low,high,size=(next_neuron_count,self.neuron_count))\n",
    "\n",
    "    def initWeightNormal(\n",
    "            self,\n",
    "            mean=5,\n",
    "            variance=3,\n",
    "            seed=0\n",
    "            ):\n",
    "        if self.next != None:\n",
    "            rng = np.random.default_rng(seed=seed)\n",
    "            next_neuron_count = self.next.neuron_count\n",
    "            self.weight = self.neuronGenerator(rng.normal,loc=mean,scale=variance*variance,size=(next_neuron_count,self.neuron_count))\n",
    "\n",
    "    def initWeightZero(\n",
    "            self\n",
    "        ):\n",
    "        if self.next != None:\n",
    "            next_neuron_count = self.next.neuron_count\n",
    "            self.weight = self.neuronGenerator(np.zeros,shape=(next_neuron_count,self.neuron_count))\n",
    "\n",
    "    def feedforward(self):\n",
    "        if self.next != None:\n",
    "            self.next.neurons = np.dot(self.neurons,self.weight.T) + self.bias_weight\n",
    "            self.next.feedforward()\n",
    "\n",
    "    def clearOutput(self):\n",
    "        Layers.layer_selector = []\n",
    "\n",
    "    def neuronGenerator(self,generator, *args, **kwargs):\n",
    "        value = generator(*args, **kwargs)\n",
    "        return np.vectorize(Neuron)(value)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNN = LayerWrapper(\n",
    "    layer_neuron_count=[3,4,2],\n",
    "    activation_function=\"linear\"\n",
    ")\n",
    "\n",
    "# layers.initWeightZero()\n",
    "# layers.neurons = np.array([3,5,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNN.layer_selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(layers.neurons,layers.weight.T)+layers.bias_weight\n",
    "\n",
    "# print(layers.bias_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
