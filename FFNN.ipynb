{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp\n",
    "from math import log\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, value, _children=(), _op='', label=''):\n",
    "        self.value = value\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children);\n",
    "        self._op = _op;\n",
    "        self.label = label # for visualization\n",
    "\n",
    "    def __repr__(self):\n",
    "        stringVal = f\"{self.value}\";\n",
    "        return stringVal\n",
    "\n",
    "    def __add__(self,other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        out = Neuron(self.value + other.value, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __mul__(self,other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        out = Neuron(self.value * other.value, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers\"\n",
    "        out = Neuron(self.value**other, (self, ), f'**{other}')\n",
    "        def _backward():\n",
    "            self.grad += other * (self.value ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        return self * other**-1\n",
    "\n",
    "    def exp(self, pos=1):\n",
    "        x = self.value*pos\n",
    "        out = Neuron(exp(x), (self, ), 'exp')\n",
    "        def _backward():\n",
    "            self.grad += out.value * out.grad * pos\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def log(self):\n",
    "        x = self.value\n",
    "        out = Neuron(log(x), (self,), 'log')\n",
    "        def _backward():\n",
    "            self.grad += (1/x) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return -(self - other)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        return other * self**-1\n",
    "    \n",
    "    def __lt__(self,other):\n",
    "        if(isinstance(other,Neuron)):\n",
    "            return self.value < other.value\n",
    "        else:\n",
    "            return self.value < other\n",
    "        \n",
    "    def __gt__(self,other):\n",
    "        if(isinstance(other,Neuron)):\n",
    "            return self.value > other.value\n",
    "        else:\n",
    "            return self.value > other\n",
    "    \n",
    "    def __le__(self,other):\n",
    "        if(isinstance(other,Neuron)):\n",
    "            return self.value <= other.value\n",
    "        else:\n",
    "            return self.value <= other\n",
    "        \n",
    "    def __ge__(self,other):\n",
    "        if(isinstance(other,Neuron)):\n",
    "            return self.value >= other.value\n",
    "        else:\n",
    "            return self.value >= other\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        topology = []\n",
    "        visited = set()\n",
    "        def build_topology(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topology(child)\n",
    "                topology.append(v)\n",
    "        build_topology(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topology):\n",
    "            node._backward()\n",
    "\n",
    "    def __float__(self):\n",
    "        return float(self.value)\n",
    "    \n",
    "    def getGrad(self):\n",
    "        return float(self.grad)\n",
    "\n",
    "    def hello(self):\n",
    "        print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper, propose better name?\n",
    "class LayerWrapper:\n",
    "    valid_loss_func = {\"mse\",\"bce\",\"cce\"}\n",
    "\n",
    "    def __init__(self,\n",
    "                 layer_neuron_width:List[int],\n",
    "                 activation_function:str=\"linear\",\n",
    "                 weight_initialization:str=\"uniform\",\n",
    "                 loss_function:str=\"mse\",\n",
    "                 uniform_parameter: tuple = (0,10,0),\n",
    "                 normal_parameter: tuple = (5,3,0),\n",
    "                 batch_size: int    = 1,\n",
    "                 learning_rate: float = 0.1,\n",
    "                 ):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_function = loss_function\n",
    "        self.layer_selector:List[Layers] = []\n",
    "        self.layer_count = len(layer_neuron_width)\n",
    "        self.loss = Neuron(0)\n",
    "        self.batch_size = batch_size\n",
    "        self.error_acumulate = Neuron(0)\n",
    "        Layers(\n",
    "            wrapper=self,\n",
    "            layer_neuron_width=layer_neuron_width,\n",
    "            activation_function=activation_function,\n",
    "            weight_initialization=weight_initialization,\n",
    "            uniform_parameter=uniform_parameter,\n",
    "            normal_parameter=normal_parameter\n",
    "        )\n",
    "\n",
    "        self.train_class_value = LayerWrapper.neuronGenerator(np.zeros,(len(self.layer_selector[-1]._neurons)))\n",
    "        self.validation_class_value = LayerWrapper.neuronGenerator(np.zeros,(len(self.layer_selector[-1]._neurons)))\n",
    "\n",
    "    # Function, dengan retval\n",
    "    @staticmethod\n",
    "    def neuronGenerator(generator, *args, **kwargs):\n",
    "        value = generator(*args, **kwargs)\n",
    "        return np.vectorize(Neuron)(value)\n",
    "    \n",
    "    def loss_func(self,Y_pred,Y_target):\n",
    "        if self.loss_function == \"mse\":\n",
    "            return LayerWrapper.meanSquareError(Y_pred,Y_target)\n",
    "        elif self.loss_function == \"bce\":\n",
    "            return LayerWrapper.binaryCrossEntropy(Y_pred,Y_target)\n",
    "        elif self.loss_function == \"cce\":\n",
    "            return LayerWrapper.categoricalCrossEntropy(Y_pred,Y_target)\n",
    "        \n",
    "    @staticmethod\n",
    "    def meanSquareError(Y_pred,Y_target):\n",
    "        tempPredictClass = Y_pred._neurons\n",
    "        tempTrueClass = Y_target\n",
    "        subtracted = np.subtract(tempTrueClass,tempPredictClass)\n",
    "        squared = np.square(subtracted)\n",
    "        return np.sum(squared)/len(tempTrueClass)\n",
    "\n",
    "    @staticmethod\n",
    "    def binaryCrossEntropy(Y_pred,Y_target):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def categoricalCrossEntropy(Y_pred,Y_target):\n",
    "        pass\n",
    "\n",
    "    def validation_feedforward(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,X_train,Y_train,X_val,Y_val):\n",
    "        pass\n",
    "\n",
    "    def predict(self,inputVal:List[float]):\n",
    "        for x,y in zip(self.layer_selector[0],inputVal):\n",
    "            x.value = y\n",
    "        self.feedforward()\n",
    "        return self.getPredResult()\n",
    "    \n",
    "    def getPredResult(self):\n",
    "        return self.layer_selector[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.layer_selector[idx]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.layer_selector}\"\n",
    "    \n",
    "\n",
    "    # Procedure, getter, setter or something else\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.layer_selector[0].feedforward()\n",
    "\n",
    "    def setTrueClassValue(self,Y:List):\n",
    "        self.train_class_value = LayerWrapper.neuronGenerator(np.array,(Y))\n",
    "        print(self.train_class_value)\n",
    "        self.layer_selector[-1].setLayerWidth(len(Y[0]))\n",
    "\n",
    "    def setValidationClassValue(self,Y):\n",
    "        self.validation_class_value = LayerWrapper.neuronGenerator(np.array,(Y))\n",
    "        self.layer_selector[-1].setLayerWidth(len(Y[0]))\n",
    "\n",
    "    def batchHelper(self,\n",
    "                    batch_input: np.ndarray,\n",
    "                    batch_class: np.ndarray,\n",
    "                    current_iter=0,\n",
    "                    ):\n",
    "        while len(batch_input) > 0:\n",
    "            if current_iter < self.batch_size:\n",
    "                self.predict(batch_input[0])\n",
    "                self.error_acumulate += self.loss_func(self.getPredResult(), batch_class[0])\n",
    "                batch_input = batch_input[1:]\n",
    "                batch_class = batch_class[1:]\n",
    "                current_iter += 1\n",
    "            else:\n",
    "                self.backpropagationLearn()\n",
    "                self.updateWeightHelper()\n",
    "                current_iter = 0\n",
    "            retval = self.error_acumulate\n",
    "            self.error_acumulate = Neuron(0)\n",
    "            return retval\n",
    "\n",
    "    def backpropagationLearn(self):\n",
    "        self.error_acumulate.backward()\n",
    "        pass\n",
    "\n",
    "    def updateWeightHelper(self):\n",
    "        self.layer_selector[0].updateWeightRecursive()\n",
    "        self.error_acumulate = Neuron(0)\n",
    "\n",
    "    # Kebodohan: backward untuk grad hanya boleh dipanggil 1x dari 1 titik, kalau kyk\n",
    "    # gini dipanggil berkali kali dan akhirnya refer ke node yg terakhir di layer \n",
    "    # terakhir\n",
    "    # *ada alasan kenapa yg dibackprop cuma errornya doang\n",
    "    # def backpropagation(self):\n",
    "    #     backprop = np.vectorize(lambda x: x.backward())\n",
    "    #     backprop(self.layer_selector[-1]._neurons)\n",
    "    # Tidak dihapus untuk catatan kebodohan\n",
    "\n",
    "\n",
    "class Layers:\n",
    "\n",
    "    # Static Attribute, cuma buat validasi input\n",
    "    valid_activation_function = {\"linear\",\"relu\",\"sigmoid\",\"hyperbolic_tangent\",\"softmax\",\"leaky_relu\",\"swish\"}\n",
    "    valid_weight_initialization = {\"zero\",\"uniform\",\"normal\"}\n",
    "\n",
    "    # START OF INITIALIZATION\n",
    "    def __init__(self,\n",
    "                 wrapper:LayerWrapper,                      # Wrapper ini buat ngepoint ke wrappernya\n",
    "                 layer_neuron_width:list,                   # List of neuron width, elemen index ke-n menentukan lebar layer ke-n\n",
    "                 activation_function:str = \"linear\",        # Harus ada di atas, kubuat defaultnya ini, dan rekursif layer selanjutnya sama\n",
    "                 weight_initialization:str = \"uniform\",     # Sama kyk activation function\n",
    "                 current_iter:int = 0,                      # jgn disentuh, ini buat nandain iterasinya, krn inisialisasi rekursif\n",
    "                 uniform_parameter: tuple = (0,10,0),       # Hanya terpakai kalau activation_functionnya uniform\n",
    "                 normal_parameter: tuple = (5,3,0),         # kyk di atas, tp normal\n",
    "                 \n",
    "                 # Tambah parameter kasih nama, named parameter biar ga broken pemanggilannya\n",
    "                 ):\n",
    "        \n",
    "        # Cek validasi\n",
    "        self.inputValidation(\n",
    "            func_name=activation_function,\n",
    "            weight_name=weight_initialization,\n",
    "            layer_neuron_width=layer_neuron_width\n",
    "            )\n",
    "        \n",
    "        self.wrapper        : LayerWrapper                      = wrapper\n",
    "        self.current_iter   : int                               = current_iter\n",
    "        self.neuron_count   : int                               = layer_neuron_width[current_iter]\n",
    "        self.bias_weight    : Neuron                            = Neuron(0)\n",
    "        self._neurons       : np.ndarray[Any, np.dtype[Neuron]] = self.neuronGenerator(np.zeros,(self.neuron_count))\n",
    "\n",
    "        self.weight_initialization  : str    = weight_initialization\n",
    "        self.uniform_parameter  : tuple = uniform_parameter\n",
    "        self.normal_parameter   : tuple = normal_parameter\n",
    "\n",
    "        self.activation_function    : str   = activation_function\n",
    "\n",
    "        wrapper.layer_selector.append(self)\n",
    "        self.next = (\n",
    "            Layers(\n",
    "                wrapper=wrapper,\n",
    "                layer_neuron_width=layer_neuron_width,\n",
    "                activation_function=activation_function,\n",
    "                current_iter=current_iter + 1,\n",
    "                weight_initialization=weight_initialization,\n",
    "                uniform_parameter=uniform_parameter,\n",
    "                normal_parameter=normal_parameter\n",
    "                )\n",
    "            if current_iter < len(layer_neuron_width) - 1\n",
    "            else None\n",
    "        )\n",
    "        self.initWeight()\n",
    "    # END OF INITIALIZATION\n",
    "\n",
    "    # Buat ngeprint\n",
    "    def __repr__(self):\n",
    "        return f\"{self._neurons}\"\n",
    "    \n",
    "    # Kadang reflek Layer[0] dan dapet error, mending lgsg keluarin Neuron nya\n",
    "    def __getitem__(self, idx):\n",
    "        return self._neurons[idx]\n",
    "\n",
    "    def __setitem__(self, idx, val):\n",
    "        if (isinstance(val,Neuron)):\n",
    "            self._neurons[idx] = val\n",
    "        else:\n",
    "            self._neurons[idx] = Neuron(val)\n",
    "\n",
    "    # Kalau ada yg aku miss tambahin aja\n",
    "    def inputValidation(self,\n",
    "                        func_name:str,\n",
    "                        weight_name:str,\n",
    "                        layer_neuron_width:list\n",
    "                        ):\n",
    "        \n",
    "        if func_name not in Layers.valid_activation_function:\n",
    "            raise ValueError(f\"Valid activation function name: {Layers.valid_activation_function}\")\n",
    "        \n",
    "        if weight_name not in Layers.valid_weight_initialization:\n",
    "            raise ValueError(f\"Valid weight initialization: {Layers.valid_weight_initialization}\")\n",
    "        \n",
    "        if len(layer_neuron_width) < 1:\n",
    "            raise ValueError(f\"layer_neuron_width must be a list with at least one positive integer.\")\n",
    "        \n",
    "        if not all(isinstance(n, int) and n > 0 for n in layer_neuron_width):\n",
    "            raise ValueError(f\"All elements in layer_neuron_width must be a positive integer\")\n",
    "\n",
    "    # Init weightnya ngikut dari named parameter    \n",
    "    def initWeight(self):\n",
    "        if self.weight_initialization==\"normal\":\n",
    "            mean        = self.normal_parameter[0]\n",
    "            variance    = self.normal_parameter[1]\n",
    "            seed        = self.normal_parameter[2]\n",
    "            self.initWeightNormal(mean,variance,seed)\n",
    "\n",
    "        elif self.weight_initialization==\"uniform\":\n",
    "            low     = self.uniform_parameter[0]\n",
    "            high    = self.uniform_parameter[1]\n",
    "            seed    = self.uniform_parameter[2]\n",
    "            self.initWeightUniform(low,high,seed)\n",
    "            \n",
    "        elif self.weight_initialization==\"zero\":\n",
    "            self.initWeightZero()\n",
    "\n",
    "    # Tiga function di bawah buat initialization, buat manual assign per layer\n",
    "    def initWeightUniform(\n",
    "            self,\n",
    "            low     : float =   0,\n",
    "            high    : float =   10,\n",
    "            seed    : int   =   0\n",
    "            ):\n",
    "        \n",
    "        if self.next != None:\n",
    "            self.weight_initialization = \"uniform\"\n",
    "            rng = np.random.default_rng(seed=seed)\n",
    "            next_neuron_count = self.next.neuron_count\n",
    "            self.weight = self.neuronGenerator(rng.uniform,low,high,size=(next_neuron_count,self.neuron_count))\n",
    "\n",
    "    def initWeightNormal(\n",
    "            self,\n",
    "            mean        :   float   =   5,\n",
    "            variance    :   float   =   3,\n",
    "            seed        :   int     =   0\n",
    "            ):\n",
    "        if self.next != None:\n",
    "            self.weight_initialization = \"normal\"\n",
    "            rng = np.random.default_rng(seed=seed)\n",
    "            next_neuron_count = self.next.neuron_count\n",
    "            self.weight = self.neuronGenerator(rng.normal,loc=mean,scale=variance*variance,size=(next_neuron_count,self.neuron_count))\n",
    "\n",
    "    def initWeightZero(\n",
    "            self\n",
    "        ):\n",
    "        if self.next != None:\n",
    "            self.weight_initialization = \"zero\"\n",
    "            next_neuron_count = self.next.neuron_count\n",
    "            self.weight = self.neuronGenerator(np.zeros,shape=(next_neuron_count,self.neuron_count))\n",
    "        \n",
    "    # Feedforward rekursif, biar ga tolol kyk sebelumnya, mohon maaf\n",
    "    def feedforward(self):\n",
    "        if self.next != None:\n",
    "            # INI BABI, GW BARU NYADAR SETELAH DEBUGGING BEBERAPA JAM, \n",
    "            # HARUSNYA CUMA REPLACE VALUE DOANG, GARA GARA NEURONNYA KEREPLACE WEIGHTNYA JG ILANG\n",
    "            self.next._neurons = np.dot(self._neurons,self.weight.T) + self.bias_weight\n",
    "            self.next._neurons = self.activate(self.next._neurons)\n",
    "            self.next.feedforward()\n",
    "\n",
    "    # Update weight\n",
    "    def updateWeightRecursive(self):\n",
    "        if self.next is not None:\n",
    "            self.weight = Layers.vectorUpdateWeight(self.weight, self.wrapper.learning_rate)\n",
    "            self.next.updateWeightRecursive()\n",
    "\n",
    "    # set Layer ini ngisi manual node nya, baru sadar ga guna jg sih\n",
    "    # node hidden layer bakal kereplace jg pas feedforward\n",
    "    def setLayerNeurons(\n",
    "            self,\n",
    "            neuron_list:List[float]\n",
    "            ):\n",
    "        self._neurons = self.neuronGenerator(np.array,(neuron_list))\n",
    "        if self.current_iter>0:\n",
    "            self.wrapper.layer_selector[self.current_iter-1].initWeight()\n",
    "        self.initWeight()\n",
    "\n",
    "    # Set lebar/jumlah neuron dari 1 layer\n",
    "    # di bawah ada set weight karena weight depend on next hidden layer width\n",
    "    def setLayerWidth(\n",
    "            self,\n",
    "            width: int = 1,\n",
    "        ):\n",
    "        self._neurons = self.neuronGenerator(np.zeros,(width))\n",
    "        self.neuron_count = width\n",
    "        if self.current_iter>0:\n",
    "            self.wrapper.layer_selector[self.current_iter-1].initWeight()\n",
    "        self.initWeight()\n",
    "\n",
    "    def setBias(self, bias:float):\n",
    "        self.bias_weight = Neuron(bias)\n",
    "\n",
    "    # Masih sama dgn kode lama, buat fill in neuron di numpy\n",
    "    def neuronGenerator(self,generator, *args, **kwargs):\n",
    "        value = generator(*args, **kwargs)\n",
    "        return np.vectorize(Neuron)(value)\n",
    "    \n",
    "    @staticmethod\n",
    "    def vectorUpdateWeight(inputVal:Neuron,rate:float):\n",
    "        def updateSingleNeuron(inputVal:Neuron,rate:float):\n",
    "            inputVal._prev = set()\n",
    "            return inputVal-(rate*inputVal.grad)\n",
    "        updateTemp = np.vectorize(updateSingleNeuron)\n",
    "        return updateTemp(inputVal,rate)\n",
    "    \n",
    "    # ACTIVATION FUNCTION\n",
    "\n",
    "    def activate(self,input):\n",
    "        if self.next.activation_function==\"linear\":\n",
    "            return Layers.linear(input)\n",
    "        elif self.next.activation_function==\"relu\":\n",
    "            return Layers.relu(input)\n",
    "        elif self.next.activation_function==\"sigmoid\":\n",
    "            return Layers.sigmoid(input)\n",
    "        elif self.next.activation_function==\"hyperbolic_tangent\":\n",
    "            return Layers.hyperbolic_tangent(input)\n",
    "        elif self.next.activation_function==\"softmax\":\n",
    "            return Layers.soft_max(input)\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear(inputVal):\n",
    "        return inputVal\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(inputVal):\n",
    "        reluTemp = np.vectorize(lambda x: x if x > 0 else Neuron(0))\n",
    "        return reluTemp(inputVal)\n",
    "    \n",
    "    # TODO: add parameter, alphanya masih hardcoded 0.5\n",
    "    def leaky_relu(self):\n",
    "        leakytemp = np.vectorize(lambda x: x if x > 0 else Neuron(0)*Neuron(0.5))\n",
    "        return leakytemp(self._neurons)\n",
    "    \n",
    "    # TODO: add parameter, beta nya masih hardcoded 1\n",
    "    def swish(self):\n",
    "        def sigmoidScalar(x:Neuron):\n",
    "            return x/(1+x.exp(-1*1))\n",
    "        \n",
    "        vectorized_sigmoid = np.vectorize(sigmoidScalar)\n",
    "        return vectorized_sigmoid(self._neurons)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(inputVal):\n",
    "        def sigmoidScalar(x:Neuron):\n",
    "            return 1/(1+x.exp(-1))\n",
    "        \n",
    "        vectorized_sigmoid = np.vectorize(sigmoidScalar)\n",
    "        return vectorized_sigmoid(inputVal)\n",
    "    \n",
    "    @staticmethod\n",
    "    def hyperbolic_tangent(inputVal):\n",
    "        def tanhScalar(x:Neuron):\n",
    "            return (x.exp()-x.exp(-1))/(x.exp()+x.exp(-1))\n",
    "        \n",
    "        vectorized_tanh = np.vectorize(tanhScalar)\n",
    "        return vectorized_tanh(inputVal)\n",
    "    \n",
    "    @staticmethod\n",
    "    def soft_max(inputVal):\n",
    "        exp = np.vectorize(lambda x: x.exp())\n",
    "        divide = np.vectorize(lambda x,y: x/y)\n",
    "        temp = exp(inputVal)\n",
    "        sumTemp = np.sum(temp)\n",
    "        return divide(temp,sumTemp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xorFunc = LayerWrapper(\n",
    "    activation_function=\"relu\",\n",
    "    weight_initialization=\"uniform\",\n",
    "    loss_function=\"mse\",\n",
    "    layer_neuron_width=[2,3,3,1],\n",
    "    batch_size=4,\n",
    "    learning_rate=0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xorFunc.setTrueClassValue([[0]])\n",
    "\n",
    "output = xorFunc.predict([1,0])\n",
    "error = xorFunc.meanSquareError(output,[[1]])\n",
    "\n",
    "# xorFunc.layer_selector[3]\n",
    "# error.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "533.9638449809597"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xorFunc.layer_selector[3][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284050.46005688846"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "error.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.369616873214543, 2.697867137638703],\n",
       "       [0.4097352393619469, 0.16527635528529094],\n",
       "       [8.132702392002724, 9.127555772777217]], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xorFunc[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "xorFunc.updateWeightHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-34772.329966535115, 2.697867137638703],\n",
       "       [-33667.71716672718, 0.16527635528529094],\n",
       "       [-23545.308317555384, 9.127555772777217]], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xorFunc[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root, format='svg', rankdir='LR'):\n",
    "    \"\"\"\n",
    "    format: png | svg | ...\n",
    "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
    "    \"\"\"\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root)\n",
    "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
    "    \n",
    "    for n in nodes:\n",
    "        dot.node(name=str(id(n)), label = \"{ data %.4f | grad %.4f }\" % (n.value, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            dot.node(name=str(id(n)) + n._op, label=n._op)\n",
    "            dot.edge(str(id(n)) + n._op, str(id(n)))\n",
    "    \n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "    \n",
    "    return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
