{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, value, _children=(), _op='', label=''):\n",
    "        self.value = value\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children);\n",
    "        self._op = _op;\n",
    "        self.label = label # for visualization\n",
    "\n",
    "    def __repr__(self):\n",
    "        stringVal = f\"{self.value}\";\n",
    "        return stringVal\n",
    "\n",
    "    def __add__(self,other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        out = Neuron(self.value + other.value, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __mul__(self,other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        out = Neuron(self.value * other.value, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers\"\n",
    "        out = Neuron(self.value**other, (self, ), f'**{other}')\n",
    "        def _backward():\n",
    "            self.grad += other * (self.value ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        return self * other**-1\n",
    "\n",
    "    def exp(self, pos=1):\n",
    "        x = self.value*pos\n",
    "        out = Neuron(exp(x), (self, ), 'exp')\n",
    "        def _backward():\n",
    "            self.grad += out.value * out.grad * pos\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return -(self - other)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        return other * self**-1\n",
    "    \n",
    "    def __lt__(self,other):\n",
    "        if(isinstance(other,Neuron)):\n",
    "            return self.value < other.value\n",
    "        else:\n",
    "            return self.value < other\n",
    "        \n",
    "    def __gt__(self,other):\n",
    "        if(isinstance(other,Neuron)):\n",
    "            return self.value > other.value\n",
    "        else:\n",
    "            return self.value > other\n",
    "    \n",
    "    def __le__(self,other):\n",
    "        if(isinstance(other,Neuron)):\n",
    "            return self.value <= other.value\n",
    "        else:\n",
    "            return self.value <= other\n",
    "        \n",
    "    def __ge__(self,other):\n",
    "        if(isinstance(other,Neuron)):\n",
    "            return self.value >= other.value\n",
    "        else:\n",
    "            return self.value >= other\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        topology = []\n",
    "        visited = set()\n",
    "        def build_topology(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topology(child)\n",
    "                topology.append(v)\n",
    "        build_topology(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topology):\n",
    "            node._backward()\n",
    "\n",
    "    def __float__(self):\n",
    "        return float(self.value)\n",
    "    \n",
    "    def getGrad(self):\n",
    "        return float(self.grad)\n",
    "\n",
    "    def hello(self):\n",
    "        print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerWrapper:\n",
    "    def __init__(self,\n",
    "                 layer_neuron_width:List[int],\n",
    "                 activation_function:str=\"linear\",\n",
    "                 weight_initialization:str=\"uniform\",\n",
    "                 uniform_parameter: tuple = (0,10,0),\n",
    "                 normal_parameter: tuple = (5,3,0)\n",
    "                 ):\n",
    "        self.layer_selector:List[Layers] = []\n",
    "        self.layer_count = len(layer_neuron_width)\n",
    "        \n",
    "        Layers(\n",
    "            wrapper=self,\n",
    "            layer_neuron_width=layer_neuron_width,\n",
    "            activation_function=activation_function,\n",
    "            weight_initialization=weight_initialization,\n",
    "            uniform_parameter=uniform_parameter,\n",
    "            normal_parameter=normal_parameter\n",
    "        )\n",
    "    \n",
    "    def feedforward(self):\n",
    "        self.layer_selector[0].feedforward()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.layer_selector[idx]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.layer_selector}\"\n",
    "    \n",
    "\n",
    "class Layers:\n",
    "\n",
    "    # Static Attribute, cuma buat validasi input\n",
    "\n",
    "    valid_activation_function = {\"linear\",\"relu\",\"sigmoid\",\"hyperbolic_tangent\",\"softmax\"}\n",
    "    valid_weight_initialization = {\"zero\",\"uniform\",\"normal\"}\n",
    "\n",
    "    # START OF INITIALIZATION\n",
    "    def __init__(self,\n",
    "                 wrapper:LayerWrapper,                      # Wrapper ini buat ngepoint ke wrappernya\n",
    "                 layer_neuron_width:list,                   # List of neuron width, elemen index ke-n menentukan lebar layer ke-n\n",
    "                 activation_function:str = \"linear\",        # Harus ada di atas, kubuat defaultnya ini, dan rekursif layer selanjutnya sama\n",
    "                 weight_initialization:str = \"uniform\",     # Sama kyk activation function\n",
    "                 current_iter:int = 0,                      # jgn disentuh, ini buat nandain iterasinya, krn inisialisasi rekursif\n",
    "                 uniform_parameter: tuple = (0,10,0),       # Hanya terpakai kalau activation_functionnya uniform\n",
    "                 normal_parameter: tuple = (5,3,0),         # kyk di atas, tp normal\n",
    "                 \n",
    "                 ):\n",
    "        \n",
    "        # Cek validasi\n",
    "        self.inputValidation(\n",
    "            func_name=activation_function,\n",
    "            weight_name=weight_initialization,\n",
    "            layer_neuron_width=layer_neuron_width\n",
    "            )\n",
    "        \n",
    "        self.wrapper        : LayerWrapper                      = wrapper\n",
    "        self.current_iter   : int                               = current_iter\n",
    "        self.neuron_count   : int                               = layer_neuron_width[current_iter]\n",
    "        self.bias_weight    : Neuron                            = Neuron(0)\n",
    "        self._neurons       : np.ndarray[Any, np.dtype[Neuron]] = self.neuronGenerator(np.zeros,(self.neuron_count))\n",
    "\n",
    "        self.weight_initialization  : str    = weight_initialization\n",
    "        self.uniform_parameter  : tuple = uniform_parameter\n",
    "        self.normal_parameter   : tuple = normal_parameter\n",
    "\n",
    "        self.activation_function    : str   = activation_function\n",
    "\n",
    "        wrapper.layer_selector.append(self)\n",
    "        self.next = (\n",
    "            Layers(\n",
    "                wrapper=wrapper,\n",
    "                layer_neuron_width=layer_neuron_width,\n",
    "                activation_function=activation_function,\n",
    "                current_iter=current_iter + 1,\n",
    "                weight_initialization=weight_initialization,\n",
    "                uniform_parameter=uniform_parameter,\n",
    "                normal_parameter=normal_parameter\n",
    "                )\n",
    "            if current_iter < len(layer_neuron_width) - 1\n",
    "            else None\n",
    "        )\n",
    "        self.initWeight()\n",
    "    # END OF INITIALIZATION\n",
    "\n",
    "    # Buat ngeprint\n",
    "    def __repr__(self):\n",
    "        return f\"{self._neurons}\"\n",
    "    \n",
    "    # Kadang reflek Layer[0] dan dapet error, mending lgsg keluarin Neuron nya\n",
    "    def __getitem__(self, idx):\n",
    "        return self._neurons[idx]\n",
    "\n",
    "    def __setitem__(self, idx, val):\n",
    "        if (isinstance(val,Neuron)):\n",
    "            self._neurons[idx] = val\n",
    "        else:\n",
    "            self._neurons[idx] = Neuron(val)\n",
    "\n",
    "    # Kalau ada yg aku miss tambahin aja\n",
    "    def inputValidation(self,\n",
    "                        func_name:str,\n",
    "                        weight_name:str,\n",
    "                        layer_neuron_width:list\n",
    "                        ):\n",
    "        \n",
    "        if func_name not in Layers.valid_activation_function:\n",
    "            raise ValueError(f\"Valid activation function name: {Layers.valid_activation_function}\")\n",
    "        \n",
    "        if weight_name not in Layers.valid_weight_initialization:\n",
    "            raise ValueError(f\"Valid weight initialization: {Layers.valid_weight_initialization}\")\n",
    "        \n",
    "        if len(layer_neuron_width) < 1:\n",
    "            raise ValueError(f\"layer_neuron_width must be a list with at least one positive integer.\")\n",
    "        \n",
    "        if not all(isinstance(n, int) and n > 0 for n in layer_neuron_width):\n",
    "            raise ValueError(f\"All elements in layer_neuron_width must be a positive integer\")\n",
    "\n",
    "    # Init weightnya ngikut dari named parameter    \n",
    "    def initWeight(self):\n",
    "        if self.weight_initialization==\"normal\":\n",
    "            mean        = self.normal_parameter[0]\n",
    "            variance    = self.normal_parameter[1]\n",
    "            seed        = self.normal_parameter[2]\n",
    "            self.initWeightNormal(mean,variance,seed)\n",
    "\n",
    "        elif self.weight_initialization==\"uniform\":\n",
    "            low     = self.uniform_parameter[0]\n",
    "            high    = self.uniform_parameter[1]\n",
    "            seed    = self.uniform_parameter[2]\n",
    "            self.initWeightUniform(low,high,seed)\n",
    "            \n",
    "        elif self.weight_initialization==\"zero\":\n",
    "            self.initWeightZero()\n",
    "\n",
    "    # Tiga function di bawah buat initialization, buat manual assign per layer\n",
    "    def initWeightUniform(\n",
    "            self,\n",
    "            low     : float =   0,\n",
    "            high    : float =   10,\n",
    "            seed    : int   =   0\n",
    "            ):\n",
    "        \n",
    "        if self.next != None:\n",
    "            self.weight_initialization = \"uniform\"\n",
    "            rng = np.random.default_rng(seed=seed)\n",
    "            next_neuron_count = self.next.neuron_count\n",
    "            self.weight = self.neuronGenerator(rng.uniform,low,high,size=(next_neuron_count,self.neuron_count))\n",
    "\n",
    "    def initWeightNormal(\n",
    "            self,\n",
    "            mean        :   float   =   5,\n",
    "            variance    :   float   =   3,\n",
    "            seed        :   int     =   0\n",
    "            ):\n",
    "        if self.next != None:\n",
    "            self.weight_initialization = \"normal\"\n",
    "            rng = np.random.default_rng(seed=seed)\n",
    "            next_neuron_count = self.next.neuron_count\n",
    "            self.weight = self.neuronGenerator(rng.normal,loc=mean,scale=variance*variance,size=(next_neuron_count,self.neuron_count))\n",
    "\n",
    "    def initWeightZero(\n",
    "            self\n",
    "        ):\n",
    "        if self.next != None:\n",
    "            self.weight_initialization = \"zero\"\n",
    "            next_neuron_count = self.next.neuron_count\n",
    "            self.weight = self.neuronGenerator(np.zeros,shape=(next_neuron_count,self.neuron_count))\n",
    "        \n",
    "    # Feedforward rekursif, biar ga tolol kyk sebelumnya, mohon maaf\n",
    "    def feedforward(self):\n",
    "        if self.next != None:\n",
    "            self.next._neurons = np.dot(self._neurons,self.weight.T) + self.bias_weight\n",
    "            self.next._neurons = self.activate()\n",
    "            self.next.feedforward()\n",
    "\n",
    "    # set Layer ini ngisi manual node nya, baru sadar ga guna jg sih\n",
    "    # node hidden layer bakal kereplace jg pas feedforward\n",
    "    def setLayerNeurons(\n",
    "            self,\n",
    "            neuron_list:List[float]\n",
    "            ):\n",
    "        self._neurons = self.neuronGenerator(np.array,(neuron_list))\n",
    "        self.neuron_count = len(self._neurons)\n",
    "        if self.current_iter>0:\n",
    "            self.wrapper.layer_selector[self.current_iter-1].initWeight()\n",
    "        self.initWeight()\n",
    "\n",
    "    # Set lebar/jumlah neuron dari 1 layer\n",
    "    # di bawah ada set weight karena weight depend on next hidden layer width\n",
    "    def setLayerWidth(\n",
    "            self,\n",
    "            width: int = 1,\n",
    "        ):\n",
    "        self._neurons = self.neuronGenerator(np.zeros,(width))\n",
    "        self.neuron_count = width\n",
    "        if self.current_iter>0:\n",
    "            self.wrapper.layer_selector[self.current_iter-1].initWeight()\n",
    "        self.initWeight()\n",
    "\n",
    "    def setBias(self, bias:float):\n",
    "        self.bias_weight = Neuron(bias)\n",
    "\n",
    "    # Masih sama dgn kode lama, buat fill in neuron di numpy\n",
    "    def neuronGenerator(self,generator, *args, **kwargs):\n",
    "        value = generator(*args, **kwargs)\n",
    "        return np.vectorize(Neuron)(value)\n",
    "    \n",
    "    # ACTIVATION FUNCTION\n",
    "\n",
    "    def activate(self):\n",
    "        if self.activation_function==\"linear\":\n",
    "            return self._neurons\n",
    "        elif self.activation_function==\"relu\":\n",
    "            return self.relu()\n",
    "        elif self.activation_function==\"sigmoid\":\n",
    "            return self.sigmoid()\n",
    "        elif self.activation_function==\"hyperbolic_tangent\":\n",
    "            return self.hyperbolicTangent()\n",
    "        elif self.activation_function==\"softmax\":\n",
    "            return self.softMax()\n",
    "\n",
    "    def relu(self):\n",
    "        reluTemp = np.vectorize(lambda x: x if x > 0 else Neuron(0))\n",
    "        return reluTemp(self._neurons)\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        def sigmoidScalar(x:Neuron):\n",
    "            return 1/(1+x.exp(-1))\n",
    "        \n",
    "        vectorized_sigmoid = np.vectorize(sigmoidScalar)\n",
    "        return vectorized_sigmoid(self._neurons)\n",
    "    \n",
    "    def hyperbolicTangent(self):\n",
    "        def tanhScalar(x:Neuron):\n",
    "            return (x.exp()-x.exp(-1))/(x.exp()+x.exp(-1))\n",
    "        \n",
    "        vectorized_tanh = np.vectorize(tanhScalar)\n",
    "        return vectorized_tanh(self._neurons)\n",
    "    \n",
    "    def softMax(self):\n",
    "        exp = np.vectorize(lambda x: x.exp())\n",
    "        divide = np.vectorize(lambda x,y: x/y)\n",
    "        temp = exp(self._neurons)\n",
    "        sumTemp = np.sum(temp)\n",
    "        return divide(temp,sumTemp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNN = LayerWrapper(\n",
    "    layer_neuron_width=[2,5,3],\n",
    "    activation_function=\"linear\",\n",
    "    weight_initialization=\"uniform\",\n",
    "    uniform_parameter=(0,10,1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNN.layer_selector[0].setLayerNeurons([-11,1])\n",
    "FFNN.layer_selector[1].setLayerWidth(3)\n",
    "FFNN.layer_selector[2].setLayerWidth(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FFNN.layer_selector[0])\n",
    "print(FFNN.layer_selector[1])\n",
    "print(FFNN.layer_selector[2])\n",
    "print(FFNN.layer_selector[0].weight)\n",
    "print(FFNN.layer_selector[1].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNN.feedforward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNN[2][0].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFNN[1]\n",
    "x = np.sum(FFNN[1]._neurons)\n",
    "x._prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FFNN[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root, format='svg', rankdir='LR'):\n",
    "    \"\"\"\n",
    "    format: png | svg | ...\n",
    "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
    "    \"\"\"\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root)\n",
    "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
    "    \n",
    "    for n in nodes:\n",
    "        dot.node(name=str(id(n)), label = \"{ data %.4f | grad %.4f }\" % (n.value, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            dot.node(name=str(id(n)) + n._op, label=n._op)\n",
    "            dot.edge(str(id(n)) + n._op, str(id(n)))\n",
    "    \n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "    \n",
    "    return dot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_scalar(x:Neuron):\n",
    "    return (1+x.exp(-1))**-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Neuron(2)\n",
    "b = sigmoid_scalar(a)\n",
    "b.backward()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softMax(array):\n",
    "        exp = np.vectorize(lambda x: x.exp())\n",
    "        divide = np.vectorize(lambda x,y: x/y)\n",
    "        temp = exp(array)\n",
    "        sumTemp = np.sum(temp)\n",
    "        return divide(temp,sumTemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Neuron(1)\n",
    "b = Neuron(3)\n",
    "c = Neuron(5)\n",
    "testArr = np.array([a,b,c])\n",
    "out = softMax(testArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
