{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, value, _children=(), _op='', label=''):\n",
    "        self.value = value\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children);\n",
    "        self._op = _op;\n",
    "        self.label = label # for visualization\n",
    "\n",
    "    def __repr__(self):\n",
    "        stringVal = f\"{self.value}\";\n",
    "        return stringVal\n",
    "\n",
    "    def __add__(self,other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        out = Neuron(self.value + other.value, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __mul__(self,other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        out = Neuron(self.value * other.value, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers\"\n",
    "        out = Neuron(self.value**other, (self, ), f'**{other}')\n",
    "        def _backward():\n",
    "            self.grad += other * (self.value ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        return self * other**-1\n",
    "\n",
    "    def exp(self, pos=1):\n",
    "        x = self.value*pos\n",
    "        out = Neuron(math.exp(x), (self, ), 'exp')\n",
    "        def _backward():\n",
    "            self.grad += out.value * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return -(self - other)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return self / other**-1\n",
    "\n",
    "    def backward(self):\n",
    "        topology = []\n",
    "        visited = set()\n",
    "        def build_topology(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topology(child)\n",
    "                topology.append(v)\n",
    "        build_topology(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topology):\n",
    "            node._backward()\n",
    "\n",
    "    # Activation function\n",
    "    def sigmoid(self):\n",
    "        x  = self.value\n",
    "        s = 1/(1+math.exp(-x))\n",
    "        out = Neuron(s, (self, ), \"sigmoid\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (s*(1-s)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def linear(self):\n",
    "        out = Neuron(self.value, (self, ), \"linear\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1*out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        x = self.value\n",
    "        r = x if x > 0 else 0\n",
    "        dr = 1 if x > 0 else 0\n",
    "        out = Neuron(r, (self, ), \"relu\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += dr*out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.value\n",
    "        t = (math.tanh(x))\n",
    "        out = Neuron(t, (self, ), \"tanh\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1-t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def softmax(self, sum):\n",
    "        return self.exp()/sum\n",
    "    \n",
    "    def swish(self):\n",
    "        x = self.value\n",
    "        sigmoid = 1/(1+math.exp(-x))\n",
    "        swish = x*sigmoid\n",
    "        out = Neuron(swish, (self, ), \"swish\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (sigmoid + x*sigmoid*(1-sigmoid)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def leakyrelu(self, alpha=0.01):\n",
    "        x = self.value\n",
    "        lr = x if x > 0 else alpha*x\n",
    "        dlr = 1 if x > 0 else alpha\n",
    "        out = Neuron(lr, (self, ), \"leakyrelu\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad +=  dlr*out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def mse(self, target, n):\n",
    "        x = self.value\n",
    "        err = (target-x)**2/n\n",
    "        out = Neuron(err, (self, ), \"mse\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += -2*(target-x)/n * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def binaryCrossEntropy(self, target, n):\n",
    "        x = self.value\n",
    "        err = -1/n*(target*math.log(x) + (1-target)*math.log(1-x))\n",
    "        out = Neuron(err, (self, ), \"bce\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += -1/n*(x-target)/(x*(1-x)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def categoricalCrossEntropy(self, target, n):\n",
    "        # Categorical Cross Entropy target will be an array cause the target will be encoded\n",
    "        # For example if we have 3 classes 0, 1, 2 and the target for this instance is 1, then the target will be [0, 1, 0]\n",
    "        # The predict or the output we get must be probability for every classes in target\n",
    "\n",
    "        x = self.value\n",
    "        c = len(target)\n",
    "        sumOut = 0\n",
    "        sumGrad = 0\n",
    "        for i in range(c):\n",
    "            sumOut += target[i]*math.log(x[i])\n",
    "            sumGrad += target[i]/x[i]\n",
    "        sumOut *= -1/n\n",
    "        sumGrad *= -1/n\n",
    "\n",
    "        out = Neuron(sum, (self, ), \"cce\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += sumGrad * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update_weight(self, learning_rate):\n",
    "        self.value = self.value-(learning_rate*self.grad)\n",
    "        self.grad = 0.0\n",
    "\n",
    "    def __float__(self):\n",
    "        return float(self.value)\n",
    "    \n",
    "    def getGrad(self):\n",
    "        return float(self.grad)\n",
    "\n",
    "    def hello(self):\n",
    "        print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN19:\n",
    "    def __init__(self,\n",
    "                 hidden_layer_count=1,\n",
    "                 input_neuron_count=1,\n",
    "                 output_neuron_count=1,\n",
    "                 activation_function=\"linear\",\n",
    "                 epoch=1,\n",
    "                 loss=\"mse\",\n",
    "                 learning_rate=0.01\n",
    "                 ):\n",
    "\n",
    "        self.hidden_layer_count             = hidden_layer_count;\n",
    "        self.input_neuron_count             = input_neuron_count;\n",
    "        self.input_layer_neuron             = self.neuronGenerator(np.array,object=[0 for i in range(self.input_neuron_count)])\n",
    "        self.output_neuron_count            = output_neuron_count;\n",
    "        self._weight_layer_count            = hidden_layer_count+1;\n",
    "        self.activation_function            = activation_function\n",
    "        self.error_layer                    = []\n",
    "        self.final_error                    = 0\n",
    "        self.epoch                          = epoch\n",
    "        self.loss                           = loss\n",
    "        self.learning_rate                  = learning_rate\n",
    "        self.weight_layers                  = [Neuron(0) for _ in range(self._weight_layer_count)];\n",
    "        self.hidden_layers                  = [Neuron(0) for _ in range(hidden_layer_count)];\n",
    "        self._activated_hidden_layers       = [Neuron(0) for _ in range(hidden_layer_count)];\n",
    "        self._output_layer                  = [Neuron(0) for _ in range(output_neuron_count)];\n",
    "        self._neuron_layer_set              = False;\n",
    "        self._weight_layers_initialized     = False;\n",
    "        self.outputs                        = []\n",
    "\n",
    "    # Vectorize is not the most efficient solution (gw mager)\n",
    "    def neuronGenerator(self,generator, *args, **kwargs):\n",
    "        value = generator(*args, **kwargs)\n",
    "        return np.vectorize(Neuron)(value)\n",
    "\n",
    "    def setHiddenLayerNeuronCount(self,*args):\n",
    "        if len(args)==0:\n",
    "            for i in range(self.hidden_layer_count):\n",
    "                self.hidden_layers[i] = self.neuronGenerator(np.zeros,(input())+1)\n",
    "                self.hidden_layers[i][0] = Neuron(1)\n",
    "            self._neuron_layer_set    = True\n",
    "        elif len(args)==1:\n",
    "            if len(args[0])!=self.hidden_layer_count:\n",
    "                raise ValueError(f\"Layer count mismatch: expected {self.hidden_layer_count} hidden layer, but {len(args[0])} were given\")\n",
    "            else:\n",
    "                for i in range(self.hidden_layer_count):\n",
    "                    self.hidden_layers[i] = self.neuronGenerator(np.zeros,args[0][i]+1)\n",
    "                    self.hidden_layers[i][0] = Neuron(1)\n",
    "                self._neuron_layer_set = True\n",
    "        else:\n",
    "            raise TypeError(f\"FFNN19.setHiddenLayerNeuronCount() takes exactly 1 positional arguments but {len(args)} were given\")\n",
    "\n",
    "    def zeroWeightInitialization(self):\n",
    "        if self._neuron_layer_set:\n",
    "            for i in range(self._weight_layer_count):\n",
    "                if i==0:\n",
    "                    self.weight_layers[i] = self.neuronGenerator(np.zeros,(self.input_neuron_count+1,len(self.hidden_layers[i])))\n",
    "                elif i==self._weight_layer_count-1:\n",
    "                    self.weight_layers[i] = self.neuronGenerator(np.zeros,(len(self.hidden_layers[i-1]),self.output_neuron_count+1))\n",
    "                else:\n",
    "                    self.weight_layers[i] = self.neuronGenerator(np.zeros,(len(self.hidden_layers[i-1]),len(self.hidden_layers[i])))\n",
    "            self._weight_layers_initialized     = True;\n",
    "        else:\n",
    "            raise ValueError(self._neuron_layer_set_message)\n",
    "\n",
    "    def uniformWeightDistribution(self,lower,upper,seed):\n",
    "        if self._neuron_layer_set:\n",
    "            rng = np.random.default_rng(seed)\n",
    "            for i in range(self._weight_layer_count):\n",
    "                if i==0:\n",
    "                    temp = self.neuronGenerator(rng.uniform,lower,upper,size=(self.input_neuron_count+1,len(self.hidden_layers[i])-1))\n",
    "                elif i==self._weight_layer_count-1:\n",
    "                    temp = self.neuronGenerator(rng.uniform,lower,upper,size=(len(self.hidden_layers[i-1]),self.output_neuron_count))\n",
    "                else:\n",
    "                    temp = self.neuronGenerator(rng.uniform,lower,upper,size=(len(self.hidden_layers[i-1]),len(self.hidden_layers[i])-1))\n",
    "                tempBias = np.full((len(temp),1), Neuron(0), dtype=object)\n",
    "                self.weight_layers[i] = np.hstack((tempBias,temp))\n",
    "            self._weight_layers_initialized     = True;\n",
    "        else:\n",
    "            raise ValueError(f\"Neuron layer count not set, please set neuron layer count before weight initialization\")\n",
    "\n",
    "    def normalWeightDistribution(self,mean,variance,seed):\n",
    "        if self._neuron_layer_set:\n",
    "            rng = np.random.default_rng(seed)\n",
    "            for i in range(self._weight_layer_count):\n",
    "                if i==0:\n",
    "                    temp = self.neuronGenerator(rng.normal,mean,variance,size=(self.input_neuron_count+1,len(self.hidden_layers[i])-1))\n",
    "                elif i==self._weight_layer_count-1:\n",
    "                    temp = self.neuronGenerator(rng.normal,mean,variance,size=(len(self.hidden_layers[i-1]),self.output_neuron_count))\n",
    "                else:\n",
    "                    temp = self.neuronGenerator(rng.normal,mean,variance,size=(len(self.hidden_layers[i-1]),len(self.hidden_layers[i])-1))\n",
    "                tempBias = np.full((len(temp),1), Neuron(0), dtype=object)\n",
    "                self.weight_layers[i] = np.hstack((tempBias,temp))\n",
    "            self._weight_layers_initialized     = True;\n",
    "        else:\n",
    "            raise ValueError(f\"Neuron layer count not set, please set neuron layer count before weight initialization\")\n",
    "\n",
    "    # Single pass, masih bingung gmn caranya buat multiple\n",
    "    def _feedforward(self,input_batch):\n",
    "        if self._weight_layers_initialized:\n",
    "            for input_data in input_batch:\n",
    "                input_data = np.insert(input_data, 0, 1)\n",
    "                # input_data.insert(0,1)\n",
    "                self.input_layer_neuron = self.neuronGenerator(np.array,object=input_data)\n",
    "                for i in range(self._weight_layer_count):\n",
    "                    if i==0:\n",
    "                        self.hidden_layers[i] = np.sum(self.input_layer_neuron*self.weight_layers[i].T,axis=1)\n",
    "                        self.hidden_layers[i] = np.vectorize(lambda n: getattr(n, self.activation_function)())(self.hidden_layers[i])\n",
    "                        # print(type(self.hidden_layers[i][1]))\n",
    "                        self.hidden_layers[i][0] = Neuron(1)\n",
    "                    elif i==self._weight_layer_count-1:\n",
    "                        self._output_layer = np.sum(self.hidden_layers[i-1]*self.weight_layers[i].T,axis=1)\n",
    "                        self._output_layer = np.vectorize(lambda n: getattr(n, self.activation_function)())(self._output_layer)\n",
    "                    else:\n",
    "                        self.hidden_layers[i] = np.sum(self.hidden_layers[i-1]*self.weight_layers[i].T,axis=1)\n",
    "                        self.hidden_layers[i] = np.vectorize(lambda n: getattr(n, self.activation_function)())(self.hidden_layers[i])\n",
    "                        self.hidden_layers[i][0] = Neuron(1)\n",
    "                self.outputs.append(self._output_layer)\n",
    "        else:\n",
    "            raise ValueError(f\"Weight layers not initialized, please initialize weight layers before feedforward\")\n",
    "    \n",
    "    def showWeightDist(self,indices):\n",
    "        for i in indices:\n",
    "            sns.displot(self.weight_layers[i][:,1:].astype(float))\n",
    "\n",
    "    def showGradDist(self,indices):\n",
    "        vecfunc = np.vectorize(Neuron.getGrad,otypes=[float])\n",
    "        for i in indices:\n",
    "            sns.displot(vecfunc(self.weight_layers[i][:,1:]))\n",
    "\n",
    "    # TODO, implementasi yg ini\n",
    "    def _backpropagation(self, train_target):\n",
    "        getattr(self, self.loss)(train_target)\n",
    "        self.final_error.backward()\n",
    "        # getattr(self, self.loss)(train_target)\n",
    "        # print(\"niga\",len(self.weight_layers))\n",
    "        # print(\"maniga\",len(self.weight_layers[0]))\n",
    "        # print(\"black man\",len(self.weight_layers[0][0]))\n",
    "        # self.weight_layers = np.vectorize(lambda n: n.update_weight(self.learning_rate))(self.weight_layers)\n",
    "        for i in self.weight_layers:\n",
    "            for j in i:\n",
    "                for k in j:\n",
    "                    # print(k)\n",
    "                    # print(\"grad:\", k.grad)\n",
    "                    k.update_weight(self.learning_rate)\n",
    "        # for i in range(len(self.weight_layers)):\n",
    "        #     for j in range(len(self.weight_layers[i])):\n",
    "        #         for k in range(len(self.weight_layers[i][j])):\n",
    "        #             # print(k)\n",
    "        #             # k = k.update_weight(self.learning_rate)\n",
    "        #             self.weight_layers[i][j][k].value = self.weight_layers[i][j][k].value-(self.learning_rate*self.weight_layers[i][j][k].grad)\n",
    "        print(\"backprop\")\n",
    "        # raise NotImplementedError(\"Belum diimplement\")\n",
    "\n",
    "    def fit(self, train_data, train_target):\n",
    "        for i in range(self.epoch):\n",
    "            print(\"=============================epoch\", i)\n",
    "            self._feedforward(train_data)\n",
    "            \n",
    "            self._backpropagation(train_target)\n",
    "            # print(self._output_layer)\n",
    "            # print(\"trtg\", train_target)\n",
    "            # getattr(self, self.loss)(train_target)\n",
    "        # raise NotImplementedError(\"Belum diimplement\")\n",
    "\n",
    "    def predict(self,data):\n",
    "        self._feedforward(data)\n",
    "        return self.outputs\n",
    "        # raise NotImplementedError(\"Belum diimplement\")\n",
    "    \n",
    "    def mse(self, target):\n",
    "        n = len(target)\n",
    "        for i in range(n):\n",
    "            self.error_layer.append(self.outputs[i][1].mse(target[i], n))\n",
    "\n",
    "        self.final_error = np.sum(self.error_layer)\n",
    "    \n",
    "    def binaryCrossEntropy(self, target):\n",
    "        n = len(target)\n",
    "        for i in range(n):\n",
    "            self.error_layer.append(self.outputs.binaryCrossEntropy(target[i], n))\n",
    "        \n",
    "        self.final_error = np.sum(self.error_layer)\n",
    "        # return -np.sum(target*np.log(output) + (np.subtract(1,target)*np.log(np.subtract(1, output))))/len(output)\n",
    "\n",
    "    def categoricalCrossEntropy(self, target):\n",
    "        n = len(target)\n",
    "        for i in range(n):\n",
    "            self.error_layer.append(self.outputs.categoricalCrossEntropy(target[i], n))\n",
    "\n",
    "        self.final_error = np.sum(self.error_layer)\n",
    "\n",
    "    def visualize(self):\n",
    "        visual = nx.DiGraph()\n",
    "        for i in range(self.input_neuron_count + 1):\n",
    "            for j in range(1,len(self.weight_layers[0][i])):\n",
    "                    tempW=float(self.weight_layers[0][i][j])\n",
    "                    tempG=float(self.weight_layers[0][i][j].grad)\n",
    "\n",
    "                    if i==0:\n",
    "                        visual.add_edge(\n",
    "                            f\"Input Bias\",\n",
    "                            self.hidden_layers[0][j],\n",
    "                            weight= f\"{tempW:.2f}\",\n",
    "                            grad=f\"{tempG:.2f}\",\n",
    "                        )\n",
    "                    else:\n",
    "                        visual.add_edge(\n",
    "                            self.input_layer_neuron[i],\n",
    "                            self.hidden_layers[0][j],\n",
    "                            weight= f\"{tempW:.2f}\",\n",
    "                            grad=f\"{tempG:.2f}\",\n",
    "                        )\n",
    "\n",
    "        for i in range(self.hidden_layer_count - 1): \n",
    "            for j in range(len(self.hidden_layers[i])):\n",
    "                for k in range(1, len(self.hidden_layers[i + 1])):\n",
    "                    tempW = float(self.weight_layers[i+1][j][k])\n",
    "                    tempG = float(self.weight_layers[i+1][j][k].grad)\n",
    "                    if j==0:\n",
    "                        visual.add_edge(\n",
    "                            f\"#Bias {i}\", \n",
    "                            self.hidden_layers[i + 1][k],\n",
    "                            weight= f\"{tempW:.2f}\",\n",
    "                            grad=f\"{tempG:.2f}\",\n",
    "                        )\n",
    "                        \n",
    "                    else:\n",
    "                        visual.add_edge(\n",
    "                            self.hidden_layers[i][j], \n",
    "                            self.hidden_layers[i + 1][k],\n",
    "                            weight= f\"{tempW:.2f}\",\n",
    "                            grad=f\"{tempG:.2f}\",\n",
    "                        )\n",
    "\n",
    "        for i in range(len(self.hidden_layers[-1])):\n",
    "            for j in range(1,len(self._output_layer)):\n",
    "                tempW = float(self.weight_layers[-1][i][j])\n",
    "                tempG = float(self.weight_layers[-1][i][j].grad)\n",
    "                if i==0:\n",
    "                    visual.add_edge(\n",
    "                        f\"Output Bias\",\n",
    "                        self._output_layer[j],\n",
    "                        weight= f\"{tempW:.2f}\",\n",
    "                        grad=f\"{tempG:.2f}\",\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    visual.add_edge(\n",
    "                        self.hidden_layers[-1][i],\n",
    "                        self._output_layer[j],\n",
    "                        weight= f\"{tempW:.2f}\",\n",
    "                        grad=f\"{tempG:.2f}\",\n",
    "                    )\n",
    "\n",
    "        pos = nx.nx_agraph.graphviz_layout(visual, prog=\"dot\", args=\"-Grankdir=LR\")\n",
    "        nx.draw(\n",
    "            visual, pos, with_labels=True, \n",
    "            node_color=\"lightblue\", edge_color=\"gray\", \n",
    "            node_size=2000, font_size=10\n",
    "        )\n",
    "        edge_weights = nx.get_edge_attributes(visual, 'weight')\n",
    "        edge_grad = nx.get_edge_attributes(visual, 'grad')\n",
    "        nx.draw_networkx_edge_labels(\n",
    "            visual, pos,\n",
    "            edge_labels=edge_weights,\n",
    "            font_color='red',\n",
    "            font_size=8,\n",
    "            label_pos=0.7\n",
    "        )\n",
    "\n",
    "        nx.draw_networkx_edge_labels(\n",
    "            visual, pos,\n",
    "            edge_labels=edge_grad,\n",
    "            font_color='blue',\n",
    "            font_size=8,\n",
    "            label_pos=0.3\n",
    "        )\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=10, test_size=10\n",
    ")\n",
    "\n",
    "model = FFNN19(\n",
    "    input_neuron_count=784,\n",
    "    hidden_layer_count=3,\n",
    "    output_neuron_count=1,\n",
    "    activation_function=\"sigmoid\",\n",
    "    epoch=5,\n",
    "    loss=\"mse\"\n",
    ")\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "# print(len(X_train))\n",
    "model.setHiddenLayerNeuronCount([4,5,8])\n",
    "model.zeroWeightInitialization()\n",
    "# model.uniformWeightDistribution(1,4,9)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test[0])\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model.weight_layers[0][1][1]))\n",
    "print(model.weight_layers)\n",
    "print(model.weight_layers[0])\n",
    "print(model.weight_layers[0][1])\n",
    "print(model.weight_layers[0][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInstance = FFNN19(\n",
    "    input_neuron_count=2,\n",
    "    hidden_layer_count=3,\n",
    "    output_neuron_count=2,\n",
    "    activation_function=\"sigmoid\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelInstance.setHiddenLayerNeuronCount([4,5,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelInstance.uniformWeightDistribution(1,4,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelInstance.setHiddenLayerNeuronCount([4,5,8])\n",
    "# modelInstance.uniformWeightDistribution(1,4,9)\n",
    "# modelInstance._feedforward([[5,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInstance._feedforward([[5,8]])\n",
    "print(1,5,7,\"\\n\")\n",
    "\n",
    "print(modelInstance.weight_layers[0],\"\\n\")\n",
    "print(modelInstance.hidden_layers[0],\"\\n\")\n",
    "print(type(modelInstance.hidden_layers[0][1]),\"\\n\")\n",
    "print(modelInstance.weight_layers[1],\"\\n\")\n",
    "print(modelInstance.hidden_layers[1],\"\\n\")\n",
    "print(modelInstance.weight_layers[2],\"\\n\")\n",
    "print(modelInstance.hidden_layers[2],\"\\n\")\n",
    "print(modelInstance.weight_layers[3],\"\\n\")\n",
    "print(modelInstance._output_layer)\n",
    "modelInstance.binaryCrossEntropy([1, 2, 4])\n",
    "print(\"error layer ini\")\n",
    "print(modelInstance.error_layer)\n",
    "modelInstance.final_error.backward()\n",
    "# print(modelInstance.binaryCrossEntropy(modelInstance._output_layer, [1, 2]))\n",
    "\n",
    "# print(modelInstance.weight_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(modelInstance.outputs[1][0]._prev)\n",
    "# modelInstance.visualize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from graphviz import Digraph\n",
    "\n",
    "# def trace(root):\n",
    "#   # builds a set of all nodes and edges in a graph\n",
    "#   nodes, edges = set(), set()\n",
    "#   def build(v):\n",
    "#     if v not in nodes:\n",
    "#       nodes.add(v)\n",
    "#       for child in v._prev:\n",
    "#         edges.add((child, v))\n",
    "#         build(child)\n",
    "#   build(root)\n",
    "#   return nodes, edges\n",
    "\n",
    "# def draw_dot(root):\n",
    "#   dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "#   nodes, edges = trace(root)\n",
    "#   for n in nodes:\n",
    "#     uid = str(id(n))\n",
    "#     # for any value in the graph, create a rectangular ('record') node for it\n",
    "#     dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.value, n.grad), shape='record')\n",
    "#     if n._op:\n",
    "#       # if this value is a result of some operation, create an op node for it\n",
    "#       dot.node(name = uid + n._op, label = n._op)\n",
    "#       # and connect this node to it\n",
    "#       dot.edge(uid + n._op, uid)\n",
    "\n",
    "#   for n1, n2 in edges:\n",
    "#     # connect n1 to the op node of n2\n",
    "#     dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "#   return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_dot(modelInstance.final_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# n_input_neuron = 3 # jumlah neuron input\n",
    "# n_input_instance = 1 # jumlah instance input\n",
    "# n_hidden_layer = 2  # jumlah layer\n",
    "# n_output_neuron = 1\n",
    "# weight_layer_count = n_hidden_layer + 1\n",
    "\n",
    "\n",
    "# input_layer_neuron = valueGenerator(np.random.randint,1,5,size=(n_input_instance,n_input_neuron))\n",
    "# output_layer_neuron = valueGenerator(np.zeros,n_output_neuron)\n",
    "# hidden_layer_neuron = [Value(0) for i in range(n_hidden_layer)]\n",
    "# self.weight_layers = [Value(0) for i in range(weight_layer_count)]\n",
    "\n",
    "# for i in range(n_hidden_layer):\n",
    "#     hidden_layer_neuron[i] = np.zeros(int(input()))\n",
    "\n",
    "# for i in range(weight_layer_count):\n",
    "#     if i==0:\n",
    "#         # weight_layer_neuron[i] = np.random.randint(5,size=(n_input_neuron,len(hidden_layer_neuron[i])))\n",
    "#         self.weight_layers[i] = valueGenerator(np.random.randint,1,6,size=(n_input_neuron,len(hidden_layer_neuron[i])))\n",
    "#     elif i==weight_layer_count-1:\n",
    "#         self.weight_layers[i] = valueGenerator(np.random.randint,1,6,size=(len(hidden_layer_neuron[i-1]),n_output_neuron))\n",
    "#         # weight_layer_neuron[i] = np.random.randint(5,size=(len(hidden_layer_neuron[i-1]),n_output_neuron))\n",
    "#     else:\n",
    "#         self.weight_layers[i] = valueGenerator(np.random.randint,1,6,size=(len(hidden_layer_neuron[i-1]),len(hidden_layer_neuron[i])))\n",
    "#         # weight_layer_neuron[i] = np.random.randint(5,size=(len(hidden_layer_neuron[i-1]),len(hidden_layer_neuron[i])))\n",
    "\n",
    "# for i in range(weight_layer_count):\n",
    "#     if i==0:\n",
    "#         hidden_layer_neuron[i] = np.sum(input_layer_neuron[0]*self.weight_layers[i].T,axis=1)\n",
    "#     elif i==weight_layer_count-1:\n",
    "#         output_layer_neuron = np.sum(hidden_layer_neuron[i-1]*self.weight_layers[i].T,axis=1)\n",
    "#     else:\n",
    "#         hidden_layer_neuron[i] = np.sum(hidden_layer_neuron[i-1]*self.weight_layers[i].T,axis=1)\n",
    "\n",
    "# print(\"Input\")\n",
    "# print(input_layer_neuron)\n",
    "# print(\"Hidden Layer\")\n",
    "# print(hidden_layer_neuron)\n",
    "# print(\"Output\")\n",
    "# print(output_layer_neuron)\n",
    "# print(\"Weight\")\n",
    "# print(self.weight_layers)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(type(self.weight_layers[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# visual = nx.DiGraph()\n",
    "\n",
    "# for i in range(modelInstance.input_neuron_count):\n",
    "#     for j in range(len(modelInstance.weight_layers[0][i])):\n",
    "#     visual.add_edge()\n",
    "\n",
    "# # visual.add_edge(1, 2, weight=1.5)\n",
    "# # visual.add_edge(1, 3, weight=2.0)\n",
    "# # visual.add_edge(2, 4, weight=0.5)\n",
    "# # visual.add_edge(3, 4, weight=1.0)\n",
    "# # visual.add_edge(4, 5, weight=2.5)\n",
    "\n",
    "# pos = nx.nx_agraph.graphviz_layout(visual, prog=\"dot\", args=\"-Grankdir=LR\")\n",
    "# nx.draw(visual, pos, with_labels=True, node_color=\"lightblue\", edge_color=\"gray\", node_size=2000, font_size=15)\n",
    "\n",
    "# edge_labels = nx.get_edge_attributes(visual, 'weight')\n",
    "# nx.draw_networkx_edge_labels(visual, pos, edge_labels=edge_labels, font_color='red')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.value, n.grad), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_dot(model.final_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInstance = FFNN19(\n",
    "    output_neuron_count=2,\n",
    "    hidden_layer_count=1,\n",
    "    input_neuron_count=2,\n",
    ")\n",
    "\n",
    "modelInstance.setHiddenLayerNeuronCount([2])\n",
    "# modelInstance.zeroWeightInitialization()\n",
    "\n",
    "modelInstance.uniformWeightDistribution(0,100,1)\n",
    "\n",
    "\n",
    "modelInstance.weight_layers[0][0][1] = Neuron(0.35)\n",
    "modelInstance.weight_layers[0][0][2] = Neuron(0.35)\n",
    "\n",
    "modelInstance.weight_layers[0][1][1] = Neuron(0.15)\n",
    "modelInstance.weight_layers[0][1][2] = Neuron(0.2)\n",
    "\n",
    "modelInstance.weight_layers[0][2][1] = Neuron(0.25)\n",
    "modelInstance.weight_layers[0][2][2] = Neuron(0.30)\n",
    "\n",
    "modelInstance.weight_layers[1][0][1] = Neuron(0.6)\n",
    "modelInstance.weight_layers[1][0][2] = Neuron(0.6)\n",
    "\n",
    "\n",
    "modelInstance.weight_layers[1][1][1] = Neuron(0.4)\n",
    "modelInstance.weight_layers[1][1][2] = Neuron(0.45)\n",
    "\n",
    "modelInstance.weight_layers[1][2][1] = Neuron(0.5)\n",
    "modelInstance.weight_layers[1][2][2] = Neuron(0.55)\n",
    "\n",
    "modelInstance.weight_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInstance._feedforward([[0.5,0.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInstance.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
