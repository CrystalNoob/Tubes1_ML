{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, value, _children=(), _op='', label=''):\n",
    "        self.value = value\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children);\n",
    "        self._op = _op;\n",
    "        self.label = label # for visualization\n",
    "\n",
    "    def __repr__(self):\n",
    "        stringVal = f\"{self.value}\";\n",
    "        return stringVal\n",
    "\n",
    "    def __add__(self,other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        out = Neuron(self.value + other.value, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __mul__(self,other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        out = Neuron(self.value * other.value, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers\"\n",
    "        out = Neuron(self.value**other, (self, ), f'**{other}')\n",
    "        def _backward():\n",
    "            self.grad += other * (self.value ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        return self * other**-1\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.value\n",
    "        out = Neuron(math.exp(x), (self, ), 'exp')\n",
    "        def _backward():\n",
    "            self.grad += out.value * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return -(self - other)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return self / other**-1\n",
    "\n",
    "    def backward(self):\n",
    "        topology = []\n",
    "        visited = set()\n",
    "        def build_topology(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topology(child)\n",
    "                topology.append(v)\n",
    "        build_topology(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topology):\n",
    "            node._backward()\n",
    "\n",
    "    def __float__(self):\n",
    "        return float(self.value)\n",
    "    \n",
    "    def getGrad(self):\n",
    "        return float(self.grad)\n",
    "\n",
    "    def hello(self):\n",
    "        print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN19:\n",
    "    def __init__(self,\n",
    "                 hidden_layer_count=1,\n",
    "                 input_neuron_count=1,\n",
    "                 output_neuron_count=1,\n",
    "                 ):\n",
    "\n",
    "        self.hidden_layer_count             = hidden_layer_count;\n",
    "        self.input_neuron_count             = input_neuron_count;\n",
    "        self.output_neuron_count            = output_neuron_count;\n",
    "        self._weight_layer_count            = hidden_layer_count+1;\n",
    "        self.weight_layers                  = [Neuron(0) for _ in range(self._weight_layer_count)];\n",
    "        self.hidden_layers                  = [Neuron(0) for _ in range(hidden_layer_count)];\n",
    "        self._activated_hidden_layers       = [Neuron(0) for _ in range(hidden_layer_count)];\n",
    "        self._output_layer                  = [Neuron(0) for _ in range(output_neuron_count)];\n",
    "        self._neuron_layer_set              = False;\n",
    "        self._weight_layers_initialized     = False;\n",
    "        self.outputs                        = []\n",
    "\n",
    "    # Vectorize is not the most efficient solution (gw mager)\n",
    "    def neuronGenerator(self,generator, *args, **kwargs):\n",
    "        value = generator(*args, **kwargs)\n",
    "        return np.vectorize(Neuron)(value)\n",
    "\n",
    "    def setHiddenLayerNeuronCount(self,*args):\n",
    "        if len(args)==0:\n",
    "            for i in range(self.hidden_layer_count):\n",
    "                self.hidden_layers[i] = self.neuronGenerator(np.zeros,(input())+1)\n",
    "                self.hidden_layers[i][0] = Neuron(1)\n",
    "            self._neuron_layer_set    = True\n",
    "        elif len(args)==1:\n",
    "            if len(args[0])!=self.hidden_layer_count:\n",
    "                raise ValueError(f\"Layer count mismatch: expected {self.hidden_layer_count} hidden layer, but {len(args[0])} were given\")\n",
    "            else:\n",
    "                for i in range(self.hidden_layer_count):\n",
    "                    self.hidden_layers[i] = self.neuronGenerator(np.zeros,args[0][i]+1)\n",
    "                    self.hidden_layers[i][0] = Neuron(1)\n",
    "                self._neuron_layer_set = True\n",
    "        else:\n",
    "            raise TypeError(f\"FFNN19.setHiddenLayerNeuronCount() takes exactly 1 positional arguments but {len(args)} were given\")\n",
    "\n",
    "    def zeroWeightInitialization(self):\n",
    "        if self._neuron_layer_set:\n",
    "            for i in range(self._weight_layer_count):\n",
    "                if i==0:\n",
    "                    self.weight_layers[i] = self.neuronGenerator(np.zeros,(self.input_neuron_count+1,len(self.hidden_layers[i])))\n",
    "                elif i==self._weight_layer_count-1:\n",
    "                    self.weight_layers[i] = self.neuronGenerator(np.zeros,(len(self.hidden_layers[i-1]),self.output_neuron_count))\n",
    "                else:\n",
    "                    self.weight_layers[i] = self.neuronGenerator(np.zeros,(len(self.hidden_layers[i-1]),len(self.hidden_layers[i])))\n",
    "            self._weight_layers_initialized     = True;\n",
    "        else:\n",
    "            raise ValueError(self._neuron_layer_set_message)\n",
    "\n",
    "    def uniformWeightDistribution(self,lower,upper,seed):\n",
    "        if self._neuron_layer_set:\n",
    "            rng = np.random.default_rng(seed)\n",
    "            for i in range(self._weight_layer_count):\n",
    "                if i==0:\n",
    "                    temp = self.neuronGenerator(rng.uniform,lower,upper,size=(self.input_neuron_count+1,len(self.hidden_layers[i])-1))\n",
    "                elif i==self._weight_layer_count-1:\n",
    "                    temp = self.neuronGenerator(rng.uniform,lower,upper,size=(len(self.hidden_layers[i-1]),self.output_neuron_count-1))\n",
    "                else:\n",
    "                    temp = self.neuronGenerator(rng.uniform,lower,upper,size=(len(self.hidden_layers[i-1]),len(self.hidden_layers[i])-1))\n",
    "                tempBias = np.full((len(temp),1), Neuron(0), dtype=object)\n",
    "                self.weight_layers[i] = np.hstack((tempBias,temp))\n",
    "            self._weight_layers_initialized     = True;\n",
    "        else:\n",
    "            raise ValueError(f\"Neuron layer count not set, please set neuron layer count before weight initialization\")\n",
    "\n",
    "    def normalWeightDistribution(self,mean,variance,seed):\n",
    "        if self._neuron_layer_set:\n",
    "            rng = np.random.default_rng(seed)\n",
    "            for i in range(self._weight_layer_count):\n",
    "                if i==0:\n",
    "                    temp = self.neuronGenerator(rng.normal,mean,variance,size=(self.input_neuron_count+1,len(self.hidden_layers[i])-1))\n",
    "                elif i==self._weight_layer_count-1:\n",
    "                    temp = self.neuronGenerator(rng.normal,mean,variance,size=(len(self.hidden_layers[i-1]),self.output_neuron_count-1))\n",
    "                else:\n",
    "                    temp = self.neuronGenerator(rng.normal,mean,variance,size=(len(self.hidden_layers[i-1]),len(self.hidden_layers[i])-1))\n",
    "                tempBias = np.full((len(temp),1), Neuron(0), dtype=object)\n",
    "                self.weight_layers[i] = np.hstack((tempBias,temp))\n",
    "            self._weight_layers_initialized     = True;\n",
    "        else:\n",
    "            raise ValueError(f\"Neuron layer count not set, please set neuron layer count before weight initialization\")\n",
    "\n",
    "    # Single pass, masih bingung gmn caranya buat multiple\n",
    "    def _feedforward(self,input_data):\n",
    "        if self._weight_layers_initialized:\n",
    "            input_data.insert(0,1)\n",
    "            input_layer_neuron = self.neuronGenerator(np.array,object=input_data)\n",
    "            for i in range(self._weight_layer_count):\n",
    "                if i==0:\n",
    "                    self.hidden_layers[i] = np.sum(input_layer_neuron*self.weight_layers[i].T,axis=1)\n",
    "                    self.hidden_layers[i][0] = Neuron(1) \n",
    "                elif i==self._weight_layer_count-1:\n",
    "                    self._output_layer = np.sum(self.hidden_layers[i-1]*self.weight_layers[i].T,axis=1)\n",
    "                else:\n",
    "                    self.hidden_layers[i] = np.sum(self.hidden_layers[i-1]*self.weight_layers[i].T,axis=1)\n",
    "                    self.hidden_layers[i][0] = Neuron(1)\n",
    "            self.outputs.append(self._output_layer)\n",
    "        else:\n",
    "            raise ValueError(f\"Weight layers not initialized, please initialize weight layers before feedforward\")\n",
    "    \n",
    "    def showWeightDist(self,indices):\n",
    "        for i in indices:\n",
    "            sns.displot(self.weight_layers[i][:,1:].astype(float))\n",
    "\n",
    "    def showGradDist(self,indices):\n",
    "        vecfunc = np.vectorize(Neuron.getGrad,otypes=[float])\n",
    "        for i in indices:\n",
    "            sns.displot(vecfunc(self.weight_layers[i][:,1:]))\n",
    "\n",
    "    # TODO, implementasi yg ini\n",
    "    def _backpropagation(self):\n",
    "        raise NotImplementedError(\"Belum diimplement\")\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        raise NotImplementedError(\"Belum diimplement\")\n",
    "\n",
    "    def predict(self,X):\n",
    "        raise NotImplementedError(\"Belum diimplement\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInstance = FFNN19(\n",
    "    input_neuron_count=2,\n",
    "    hidden_layer_count=3,\n",
    "    output_neuron_count=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInstance.setHiddenLayerNeuronCount([4,5,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInstance.uniformWeightDistribution(1,4,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInstance.weight_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInstance._feedforward([5,8])\n",
    "print(1,5,7,\"\\n\")\n",
    "\n",
    "print(modelInstance.weight_layers[0],\"\\n\")\n",
    "print(modelInstance.hidden_layers[0],\"\\n\")\n",
    "print(modelInstance.weight_layers[1],\"\\n\")\n",
    "print(modelInstance.hidden_layers[1],\"\\n\")\n",
    "print(modelInstance.weight_layers[2],\"\\n\")\n",
    "print(modelInstance.hidden_layers[2],\"\\n\")\n",
    "print(modelInstance.weight_layers[3],\"\\n\")\n",
    "print(modelInstance._output_layer)\n",
    "\n",
    "# print(modelInstance.weight_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# n_input_neuron = 3 # jumlah neuron input\n",
    "# n_input_instance = 1 # jumlah instance input\n",
    "# n_hidden_layer = 2  # jumlah layer\n",
    "# n_output_neuron = 1\n",
    "# weight_layer_count = n_hidden_layer + 1\n",
    "\n",
    "\n",
    "# input_layer_neuron = valueGenerator(np.random.randint,1,5,size=(n_input_instance,n_input_neuron))\n",
    "# output_layer_neuron = valueGenerator(np.zeros,n_output_neuron)\n",
    "# hidden_layer_neuron = [Value(0) for i in range(n_hidden_layer)]\n",
    "# self.weight_layers = [Value(0) for i in range(weight_layer_count)]\n",
    "\n",
    "# for i in range(n_hidden_layer):\n",
    "#     hidden_layer_neuron[i] = np.zeros(int(input()))\n",
    "\n",
    "# for i in range(weight_layer_count):\n",
    "#     if i==0:\n",
    "#         # weight_layer_neuron[i] = np.random.randint(5,size=(n_input_neuron,len(hidden_layer_neuron[i])))\n",
    "#         self.weight_layers[i] = valueGenerator(np.random.randint,1,6,size=(n_input_neuron,len(hidden_layer_neuron[i])))\n",
    "#     elif i==weight_layer_count-1:\n",
    "#         self.weight_layers[i] = valueGenerator(np.random.randint,1,6,size=(len(hidden_layer_neuron[i-1]),n_output_neuron))\n",
    "#         # weight_layer_neuron[i] = np.random.randint(5,size=(len(hidden_layer_neuron[i-1]),n_output_neuron))\n",
    "#     else:\n",
    "#         self.weight_layers[i] = valueGenerator(np.random.randint,1,6,size=(len(hidden_layer_neuron[i-1]),len(hidden_layer_neuron[i])))\n",
    "#         # weight_layer_neuron[i] = np.random.randint(5,size=(len(hidden_layer_neuron[i-1]),len(hidden_layer_neuron[i])))\n",
    "\n",
    "# for i in range(weight_layer_count):\n",
    "#     if i==0:\n",
    "#         hidden_layer_neuron[i] = np.sum(input_layer_neuron[0]*self.weight_layers[i].T,axis=1)\n",
    "#     elif i==weight_layer_count-1:\n",
    "#         output_layer_neuron = np.sum(hidden_layer_neuron[i-1]*self.weight_layers[i].T,axis=1)\n",
    "#     else:\n",
    "#         hidden_layer_neuron[i] = np.sum(hidden_layer_neuron[i-1]*self.weight_layers[i].T,axis=1)\n",
    "\n",
    "# print(\"Input\")\n",
    "# print(input_layer_neuron)\n",
    "# print(\"Hidden Layer\")\n",
    "# print(hidden_layer_neuron)\n",
    "# print(\"Output\")\n",
    "# print(output_layer_neuron)\n",
    "# print(\"Weight\")\n",
    "# print(self.weight_layers)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(type(self.weight_layers[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
