{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, value, _children=(), _op='', label=''):\n",
    "        self.value = value\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children);\n",
    "        self._op = _op;\n",
    "        self.label = label # for visualization\n",
    "\n",
    "    def __repr__(self):\n",
    "        stringVal = f\"{self.value}\";\n",
    "        return stringVal\n",
    "\n",
    "    def __add__(self,other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        out = Neuron(self.value + other.value, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __mul__(self,other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        out = Neuron(self.value * other.value, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.grad += other.value * out.grad\n",
    "            other.grad += self.value * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers\"\n",
    "        out = Neuron(self.value**other, (self, ), f'**{other}')\n",
    "        def _backward():\n",
    "            self.grad += other * (self.value ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        other = other if isinstance(other, Neuron) else Neuron(other)\n",
    "        return self * other**-1\n",
    "\n",
    "    def exp(self, pos=1):\n",
    "        x = self.value*pos\n",
    "        out = Neuron(math.exp(x), (self, ), 'exp')\n",
    "        def _backward():\n",
    "            self.grad += out.value * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return -(self - other)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return self / other**-1\n",
    "\n",
    "    def backward(self):\n",
    "        topology = []\n",
    "        visited = set()\n",
    "        def build_topology(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topology(child)\n",
    "                topology.append(v)\n",
    "        build_topology(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topology):\n",
    "            node._backward()\n",
    "\n",
    "    # Activation function\n",
    "    def sigmoid(self):\n",
    "        x  = self.value\n",
    "        s = 1/(1+math.exp(-x))\n",
    "        out = Neuron(s, (self, ), \"sigmoid\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (s*(1-s)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def linear(self):\n",
    "        out = Neuron(self.value, (self, ), \"linear\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1*out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        x = self.value\n",
    "        r = x if x > 0 else 0\n",
    "        dr = 1 if x > 0 else 0\n",
    "        out = Neuron(r, (self, ), \"relu\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += dr*out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.value\n",
    "        t = (math.tanh(x))\n",
    "        out = Neuron(t, (self, ), \"tanh\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1-t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def softmax(self, sum):\n",
    "        return self.exp()/sum\n",
    "    \n",
    "    def swish(self):\n",
    "        x = self.value\n",
    "        sigmoid = 1/(1+math.exp(-x))\n",
    "        swish = x*sigmoid\n",
    "        out = Neuron(swish, (self, ), \"swish\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (sigmoid + x*sigmoid*(1-sigmoid)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def leakyrelu(self, alpha=0.01):\n",
    "        x = self.value\n",
    "        lr = x if x > 0 else alpha*x\n",
    "        dlr = 1 if x > 0 else alpha\n",
    "        out = Neuron(lr, (self, ), \"leakyrelu\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad +=  dlr*out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def hello():\n",
    "        print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN19:\n",
    "    def __init__(self,\n",
    "                 hidden_layer_count=1,\n",
    "                 input_neuron_count=1,\n",
    "                 output_neuron_count=1,\n",
    "                 activation_function=\"linear\"\n",
    "                 ):\n",
    "\n",
    "        self.hidden_layer_count             = hidden_layer_count;\n",
    "        self.input_neuron_count             = input_neuron_count;\n",
    "        self.output_neuron_count            = output_neuron_count;\n",
    "        self._weight_layer_count            = hidden_layer_count+1;\n",
    "        self.activation_function            = activation_function\n",
    "        self.weight_layers                  = [Neuron(0) for _ in range(self._weight_layer_count)];\n",
    "        self.hidden_layers                  = [Neuron(0) for _ in range(hidden_layer_count)];\n",
    "        self._activated_hidden_layers       = [Neuron(0) for _ in range(hidden_layer_count)];\n",
    "        self._output_layer                  = [Neuron(0) for _ in range(output_neuron_count)];\n",
    "        self._neuron_layer_set              = False;\n",
    "        self._weight_layers_initialized     = False;\n",
    "        self.outputs                        = []\n",
    "\n",
    "    # Vectorize is not the most efficient solution (gw mager)\n",
    "    def neuronGenerator(self,generator, *args, **kwargs):\n",
    "        value = generator(*args, **kwargs)\n",
    "        return np.vectorize(Neuron)(value)\n",
    "\n",
    "    def setHiddenLayerNeuronCount(self,*args):\n",
    "        if len(args)==0:\n",
    "            for i in range(self.hidden_layer_count):\n",
    "                self.hidden_layers[i] = self.neuronGenerator(np.zeros,(input())+1)\n",
    "                self.hidden_layers[i][0] = Neuron(1)\n",
    "            self._neuron_layer_set    = True\n",
    "        elif len(args)==1:\n",
    "            if len(args[0])!=self.hidden_layer_count:\n",
    "                raise ValueError(f\"Layer count mismatch: expected {self.hidden_layer_count} hidden layer, but {len(args[0])} were given\")\n",
    "            else:\n",
    "                for i in range(self.hidden_layer_count):\n",
    "                    self.hidden_layers[i] = self.neuronGenerator(np.zeros,args[0][i]+1)\n",
    "                    self.hidden_layers[i][0] = Neuron(1)\n",
    "                self._neuron_layer_set = True\n",
    "        else:\n",
    "            raise TypeError(f\"FFNN19.setHiddenLayerNeuronCount() takes exactly 1 positional arguments but {len(args)} were given\")\n",
    "\n",
    "    def zeroWeightInitialization(self):\n",
    "        if self._neuron_layer_set:\n",
    "            for i in range(self._weight_layer_count):\n",
    "                if i==0:\n",
    "                    self.weight_layers[i] = self.neuronGenerator(np.zeros,(self.input_neuron_count+1,len(self.hidden_layers[i])))\n",
    "                elif i==self._weight_layer_count-1:\n",
    "                    self.weight_layers[i] = self.neuronGenerator(np.zeros,(len(self.hidden_layers[i-1]),self.output_neuron_count))\n",
    "                else:\n",
    "                    self.weight_layers[i] = self.neuronGenerator(np.zeros,(len(self.hidden_layers[i-1]),len(self.hidden_layers[i])))\n",
    "            self._weight_layers_initialized     = True;\n",
    "        else:\n",
    "            raise ValueError(self._neuron_layer_set_message)\n",
    "\n",
    "    def uniformWeightDistribution(self,lower,upper,seed):\n",
    "        if self._neuron_layer_set:\n",
    "            rng = np.random.default_rng(seed)\n",
    "            for i in range(self._weight_layer_count):\n",
    "                if i==0:\n",
    "                    self.weight_layers[i] = self.neuronGenerator(rng.uniform,lower,upper,size=(self.input_neuron_count+1,len(self.hidden_layers[i])))\n",
    "                elif i==self._weight_layer_count-1:\n",
    "                    self.weight_layers[i] = self.neuronGenerator(rng.uniform,lower,upper,size=(len(self.hidden_layers[i-1]),self.output_neuron_count))\n",
    "                else:\n",
    "                    self.weight_layers[i] = self.neuronGenerator(rng.uniform,lower,upper,size=(len(self.hidden_layers[i-1]),len(self.hidden_layers[i])))\n",
    "                if i!=self._weight_layer_count-1:\n",
    "                    self.weight_layers[i][:,0] = Neuron(0)\n",
    "            self._weight_layers_initialized     = True;\n",
    "        else:\n",
    "            raise ValueError(f\"Neuron layer count not set, please set neuron layer count before weight initialization\")\n",
    "\n",
    "    def normalWeightDistribution(self,mean,variance,seed):\n",
    "        if self._neuron_layer_set:\n",
    "            rng = np.random.default_rng(seed)\n",
    "            for i in range(self._weight_layer_count):\n",
    "                if i==0:\n",
    "                    self.weight_layers[i] = self.neuronGenerator(rng.normal,mean,variance,size=(self.input_neuron_count+1,len(self.hidden_layers[i])))\n",
    "                elif i==self._weight_layer_count-1:\n",
    "                    self.weight_layers[i] = self.neuronGenerator(rng.normal,mean,variance,size=(len(self.hidden_layers[i-1]),self.output_neuron_count))\n",
    "                else:\n",
    "                    self.weight_layers[i] = self.neuronGenerator(rng.normal,mean,variance,size=(len(self.hidden_layers[i-1]),len(self.hidden_layers[i])))\n",
    "                if i!=self._weight_layer_count-1:\n",
    "                    self.weight_layers[i][:,0] = Neuron(0)\n",
    "            self._weight_layers_initialized     = True;\n",
    "        else:\n",
    "            raise ValueError(f\"Neuron layer count not set, please set neuron layer count before weight initialization\")\n",
    "\n",
    "    # Single pass, masih bingung gmn caranya buat multiple\n",
    "    def _feedforward(self,input_data):\n",
    "        if self._weight_layers_initialized:\n",
    "            input_data.insert(0,1)\n",
    "            input_layer_neuron = self.neuronGenerator(np.array,object=input_data)\n",
    "            for i in range(self._weight_layer_count):\n",
    "                if i==0:\n",
    "                    self.hidden_layers[i] = np.sum(input_layer_neuron*self.weight_layers[i].T,axis=1)\n",
    "                    # self.hidden_layers[i] = np.vectorize(lambda n: n.tanh())(self.hidden_layers[i])\n",
    "                    self.hidden_layers[i] = np.vectorize(lambda n: getattr(n, self.activation_function)())(self.hidden_layers[i])\n",
    "                    print(type(self.hidden_layers[i][1]))\n",
    "                    self.hidden_layers[i][0] = Neuron(1)\n",
    "                elif i==self._weight_layer_count-1:\n",
    "                    self._output_layer = np.sum(self.hidden_layers[i-1]*self.weight_layers[i].T,axis=1)\n",
    "                    # self._output_layer = np.vectorize(lambda n: n.sigmoid())(self._output_layer)\n",
    "                    self._output_layer = np.vectorize(lambda n: getattr(n, self.activation_function)())(self._output_layer)\n",
    "                else:\n",
    "                    self.hidden_layers[i] = np.sum(self.hidden_layers[i-1]*self.weight_layers[i].T,axis=1)\n",
    "                    # self.hidden_layers[i] = np.vectorize(lambda n: n.sigmoid())(self.hidden_layers[i])\n",
    "                    self.hidden_layers[i] = np.vectorize(lambda n: getattr(n, self.activation_function)())(self.hidden_layers[i])\n",
    "                    self.hidden_layers[i][0] = Neuron(1)\n",
    "            self.outputs.append(self._output_layer)\n",
    "        else:\n",
    "            raise ValueError(f\"Weight layers not initialized, please initialize weight layers before feedforward\")\n",
    "\n",
    "    # TODO, implementasi yg ini\n",
    "    def _backpropagation(self):\n",
    "        raise NotImplementedError(\"Belum diimplement\")\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        raise NotImplementedError(\"Belum diimplement\")\n",
    "\n",
    "    def predict(self,X):\n",
    "        raise NotImplementedError(\"Belum diimplement\")\n",
    "    \n",
    "    def mse(self, output, target):\n",
    "        return np.sum(np.power(target-output, 2))/len(output)\n",
    "    \n",
    "    def binaryCrossEntropy(self, output, target):\n",
    "        return -np.sum(target*np.log(output) + (np.subtract(1,target)*np.log(np.subtract(1, output))))/len(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInstance = FFNN19(\n",
    "    input_neuron_count=2,\n",
    "    hidden_layer_count=3,\n",
    "    output_neuron_count=2,\n",
    "    activation_function=\"sigmoid\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInstance.setHiddenLayerNeuronCount([4,5,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelInstance.uniformWeightDistribution(1,4,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Neuron'>\n",
      "1 5 7 \n",
      "\n",
      "[[0 1.860451627262666 2.8094444501546856 3.3326022487605362\n",
      "  3.148223888810696]\n",
      " [0 3.581180947548654 3.754712887146287 1.0797632034027917\n",
      "  2.311744000388526]\n",
      " [0 1.1954625921103226 1.0168781520381787 3.491864310913771\n",
      "  3.9499067328585267]] \n",
      "\n",
      "[1 1.000000000000183 1.0000000000001241 1.0000000000000002 1.0] \n",
      "\n",
      "<class '__main__.Neuron'> \n",
      "\n",
      "[[0 1.9468104699348194 3.116001113768361 1.8975432195947401\n",
      "  3.222245448627246 1.8392416592448608]\n",
      " [0 3.96322224068373 3.9587156478407155 3.6485877009900274\n",
      "  3.738430155351754 3.1246207423095176]\n",
      " [0 3.7697146242106614 1.2691923395629376 2.1103946354200223\n",
      "  2.817816830527789 2.433189690756057]\n",
      " [0 1.5852591804371396 1.1867604922499901 1.3795552217197549\n",
      "  3.888344754751488 1.3983236116882913]\n",
      " [0 2.131169112868656 2.8702649240223876 3.3462883920079083\n",
      "  1.2359466813078437 3.7937123594291053]] \n",
      "\n",
      "[1 1.0000015209496809 1.0000041147416123 1.0000041918467484\n",
      " 1.0000003371345019 1.0000034090119363] \n",
      "\n",
      "[[0 1.9628923531729197 1.567996058284693 2.7878586633610993\n",
      "  2.784067680284469 1.8024050764914379 1.5081784391690176\n",
      "  3.299401772440474 3.018025757742162]\n",
      " [0 1.6587300743492903 3.1696255886443145 1.2238141387145487\n",
      "  1.0220139833780832 1.5383035770702809 3.346128016182113\n",
      "  2.620208391173403 2.8501150253984706]\n",
      " [0 2.9076493884651065 1.9687173836529823 3.161809995739726\n",
      "  1.0439995700539741 2.131659410959805 1.1639917021797104\n",
      "  1.377467723875556 3.713763309127099]\n",
      " [0 3.372792864227332 2.817159079405261 2.715672708085035\n",
      "  3.3848976183717685 2.8694499532338424 2.530943802356545\n",
      "  2.6696286689636923 2.7067517035738557]\n",
      " [0 3.003276811578243 2.22059681774956 2.963551939046561\n",
      "  2.454320269024359 3.245302695291288 3.079182125950079\n",
      "  1.1183381695521806 3.346468346421908]\n",
      " [0 1.9930921941751307 3.1987889401882152 3.5358874807396035\n",
      "  2.4964228148920675 1.3872536305056715 3.1943790698289387\n",
      "  3.5160648550273357 1.9346011306522135]] \n",
      "\n",
      "[1 1.000000338591958 1.0000003238710962 1.0000000762968462\n",
      " 1.0000018771585895 1.000002318933199 1.0000003651945646\n",
      " 1.0000004558314202 1.0000000234179278] \n",
      "\n",
      "[[1.6556609925936483 3.2999838642924013]\n",
      " [2.548519552852035 1.0450585486094544]\n",
      " [1.586918586298792 1.6520256191832796]\n",
      " [2.7448570202847202 2.6770977232390445]\n",
      " [2.5511599761289703 2.0231572459491414]\n",
      " [2.177175078292466 2.5012039815353724]\n",
      " [2.706563021813197 2.6951012808589305]\n",
      " [3.1463560544167835 3.982536583840241]\n",
      " [3.24646799156485 2.4716540047847233]] \n",
      "\n",
      "[1.0000000001938973 1.000000000196997]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type Neuron which has no callable log method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: 'Neuron' object has no attribute 'log'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[732], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(modelInstance\u001b[38;5;241m.\u001b[39mweight_layers[\u001b[38;5;241m3\u001b[39m],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(modelInstance\u001b[38;5;241m.\u001b[39m_output_layer)\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodelInstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinaryCrossEntropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelInstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# print(modelInstance.weight_layers)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[728], line 128\u001b[0m, in \u001b[0;36mFFNN19.binaryCrossEntropy\u001b[1;34m(self, output, target)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbinaryCrossEntropy\u001b[39m(\u001b[38;5;28mself\u001b[39m, output, target):\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(target\u001b[38;5;241m*\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m (np\u001b[38;5;241m.\u001b[39msubtract(\u001b[38;5;241m1\u001b[39m,target)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(np\u001b[38;5;241m.\u001b[39msubtract(\u001b[38;5;241m1\u001b[39m, output))))\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(output)\n",
      "\u001b[1;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type Neuron which has no callable log method"
     ]
    }
   ],
   "source": [
    "modelInstance._feedforward([5,8])\n",
    "print(1,5,7,\"\\n\")\n",
    "\n",
    "print(modelInstance.weight_layers[0],\"\\n\")\n",
    "print(modelInstance.hidden_layers[0],\"\\n\")\n",
    "print(type(modelInstance.hidden_layers[0][1]),\"\\n\")\n",
    "print(modelInstance.weight_layers[1],\"\\n\")\n",
    "print(modelInstance.hidden_layers[1],\"\\n\")\n",
    "print(modelInstance.weight_layers[2],\"\\n\")\n",
    "print(modelInstance.hidden_layers[2],\"\\n\")\n",
    "print(modelInstance.weight_layers[3],\"\\n\")\n",
    "print(modelInstance._output_layer)\n",
    "print(modelInstance.binaryCrossEntropy(modelInstance._output_layer, [1, 2]))\n",
    "\n",
    "# print(modelInstance.weight_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.0000000001938973, 1.000000000196997], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "print(modelInstance.outputs)\n",
    "# print(modelInstance.outputs[1][0]._prev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# n_input_neuron = 3 # jumlah neuron input\n",
    "# n_input_instance = 1 # jumlah instance input\n",
    "# n_hidden_layer = 2  # jumlah layer\n",
    "# n_output_neuron = 1\n",
    "# weight_layer_count = n_hidden_layer + 1\n",
    "\n",
    "\n",
    "# input_layer_neuron = valueGenerator(np.random.randint,1,5,size=(n_input_instance,n_input_neuron))\n",
    "# output_layer_neuron = valueGenerator(np.zeros,n_output_neuron)\n",
    "# hidden_layer_neuron = [Value(0) for i in range(n_hidden_layer)]\n",
    "# self.weight_layers = [Value(0) for i in range(weight_layer_count)]\n",
    "\n",
    "# for i in range(n_hidden_layer):\n",
    "#     hidden_layer_neuron[i] = np.zeros(int(input()))\n",
    "\n",
    "# for i in range(weight_layer_count):\n",
    "#     if i==0:\n",
    "#         # weight_layer_neuron[i] = np.random.randint(5,size=(n_input_neuron,len(hidden_layer_neuron[i])))\n",
    "#         self.weight_layers[i] = valueGenerator(np.random.randint,1,6,size=(n_input_neuron,len(hidden_layer_neuron[i])))\n",
    "#     elif i==weight_layer_count-1:\n",
    "#         self.weight_layers[i] = valueGenerator(np.random.randint,1,6,size=(len(hidden_layer_neuron[i-1]),n_output_neuron))\n",
    "#         # weight_layer_neuron[i] = np.random.randint(5,size=(len(hidden_layer_neuron[i-1]),n_output_neuron))\n",
    "#     else:\n",
    "#         self.weight_layers[i] = valueGenerator(np.random.randint,1,6,size=(len(hidden_layer_neuron[i-1]),len(hidden_layer_neuron[i])))\n",
    "#         # weight_layer_neuron[i] = np.random.randint(5,size=(len(hidden_layer_neuron[i-1]),len(hidden_layer_neuron[i])))\n",
    "\n",
    "# for i in range(weight_layer_count):\n",
    "#     if i==0:\n",
    "#         hidden_layer_neuron[i] = np.sum(input_layer_neuron[0]*self.weight_layers[i].T,axis=1)\n",
    "#     elif i==weight_layer_count-1:\n",
    "#         output_layer_neuron = np.sum(hidden_layer_neuron[i-1]*self.weight_layers[i].T,axis=1)\n",
    "#     else:\n",
    "#         hidden_layer_neuron[i] = np.sum(hidden_layer_neuron[i-1]*self.weight_layers[i].T,axis=1)\n",
    "\n",
    "# print(\"Input\")\n",
    "# print(input_layer_neuron)\n",
    "# print(\"Hidden Layer\")\n",
    "# print(hidden_layer_neuron)\n",
    "# print(\"Output\")\n",
    "# print(output_layer_neuron)\n",
    "# print(\"Weight\")\n",
    "# print(self.weight_layers)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(type(self.weight_layers[0][0][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
